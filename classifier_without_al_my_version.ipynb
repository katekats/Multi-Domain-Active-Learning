{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier without Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains all results of the classifier without Active Learning. For this, execute all cells until the headline \"Model training of the proposed model and baseline model\" is reached. Then, execute one of the models: Either one of the models under \"Model training of the proposed model and baseline models\" or one of the models under \"Experiments\". Note: Make sure, that the desired target domain is chosen when loading the data below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to adjust the checkpoint paths when training the models such that the weights are saved in the desired paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and setting configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_self_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_554/1346675604.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#from keras.models import Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#from keras.layers import Input,Dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_self_attention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeqSelfAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_self_attention'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "\n",
    "#from keras.models import Model\n",
    "#from keras.layers import Input,Dense\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seeds in order to reproduce the results\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(1)\n",
    "rn.seed(2)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# configurations so we use a single thread\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to load the data of the desired target domain here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1610,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data for the general sentence embeddings, here corresponding data from domain 0 was chosen\n",
    "with open('data/sentence_embeddings/general/sorted/train/train_data2_2.p', 'rb') as f:\n",
    "    X_train_gen = pkl.load(f)\n",
    "\n",
    "with open('data/sentence_embeddings/general/sorted/train/train_labels2_2.p', 'rb') as f:\n",
    "    y_train = pkl.load(f)\n",
    "    \n",
    "with open('data/sentence_embeddings/general/sorted/val_test/vt_data2_2.p', 'rb') as f:\n",
    "    X_val_test_spec = pkl.load(f)\n",
    "\n",
    "with open('data/sentence_embeddings/general/sorted/val_test/vt_labels2_2.p', 'rb') as f:\n",
    "    y_val_test = pkl.load(f)\n",
    "\n",
    "labels_total = np.hstack((y_train[:,:1400], y_val_test))\n",
    "X_train_gen, X_val_gen, X_test_gen = X_train_gen[:1400], X_val_test_spec[:200], X_val_test_spec[200:]\n",
    "y_train, y_val, y_test = y_train[0,:1400], y_val_test[0,:200], y_val_test[0,200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6000)"
      ]
     },
     "execution_count": 2122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2131,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/sentence_embeddings/general/sorted/train/train_data5_8.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2131-92c3e062ae3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# importing the data for the general sentence embeddings, here corresponding data from domain 0 was chosen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/sentence_embeddings/general/sorted/train/train_data5_8.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mX_train_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/sentence_embeddings/general/sorted/train/train_data5_8.p'"
     ]
    }
   ],
   "source": [
    "# set the target domain\n",
    "index_spec = 5\n",
    "\n",
    "\n",
    "# importing the data for the general sentence embeddings, here corresponding data from domain 0 was chosen\n",
    "with open('data/sentence_embeddings/general/sorted/train/train_data5_8.p', 'rb') as f:\n",
    "    X_train_gen = pkl.load(f)\n",
    "\n",
    "with open('data/sentence_embeddings/general/sorted/train/train_labels5_8.p', 'rb') as f:\n",
    "    y_train = pkl.load(f)\n",
    "    \n",
    "with open('data/sentence_embeddings/general/sorted/val_test/vt_data5_8.p', 'rb') as f:\n",
    "    X_val_test_spec = pkl.load(f)\n",
    "\n",
    "with open('data/sentence_embeddings/general/sorted/val_test/vt_labels5_8.p', 'rb') as f:\n",
    "    y_val_test = pkl.load(f)\n",
    "\n",
    "\n",
    "labels_total = np.hstack((y_train[:,:4200], y_val_test))\n",
    "X_train_gen, X_val_gen, X_test_gen = X_train_gen[:4200], X_val_test_spec[:600], X_val_test_spec[600:]\n",
    "y_train, y_val, y_test = y_train[0,:4200], y_val_test[0,:600], y_val_test[0,600:]\n",
    "\n",
    "# import the data from the specific sentence embeddings, here corresponding data from domain 0 was chosen\n",
    "with open('data/sentence_embeddings/specific/sentemb/sentemb_unlabeled5_8.p', 'rb') as f:\n",
    "    X_spec = pkl.load(f)\n",
    "    \n",
    "#X_train_spec, X_val_spec, X_test_spec = X_spec[:1400], X_spec[1400:1600], X_spec[1600:2000] \n",
    "\n",
    "import numpy as np\n",
    "X_spec=np.repeat(X_spec,repeats=3, axis=1)\n",
    "\n",
    "X_train_spec, X_val_spec, X_test_spec = X_spec.transpose()[:4200], X_spec.transpose()[4200:4800], X_spec.transpose()[4800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training of proposed model and baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose one fo the three models below to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the proposed classifier\n",
    "\n",
    "INPUT_SIZE = 300\n",
    "LATENT_SIZE = 300\n",
    "\n",
    "# domain-general model parts\n",
    "inp_gen = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "inp_gen_att, attn_weights_gen = SeqSelfAttention(return_attention = True)(inp_gen)\n",
    "out_gen = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_gen_att)\n",
    "\n",
    "# domain-specific model parts\n",
    "inp_spec = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "inp_spec_att, attn_weights_spec = SeqSelfAttention(return_attention = True)(inp_spec)\n",
    "out_spec = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_spec_att)\n",
    "\n",
    "# concatenate domain-general and domain-specific results\n",
    "merged = tf.keras.layers.Concatenate()([out_gen, out_spec])\n",
    "\n",
    "# drop out layer and dense layer\n",
    "merged = tf.keras.layers.Dropout(.5)(merged)\n",
    "merged = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "classifier = tf.keras.Model([inp_gen,inp_spec], merged)\n",
    "#classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the proposed classifier\n",
    "\n",
    "INPUT_SIZE = 300\n",
    "LATENT_SIZE = 300\n",
    "\n",
    "# domain-general model parts\n",
    "inp_gen = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "#inp_gen_att, attn_weights_gen = SeqSelfAttention(return_attention = True)(inp_gen)\n",
    "#out_gen = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_gen)\n",
    "\n",
    "# domain-specific model parts\n",
    "inp_spec = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "#inp_spec_att, attn_weights_spec = SeqSelfAttention(return_attention = True)(inp_spec)\n",
    "#out_spec = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_spec)\n",
    "\n",
    "# concatenate domain-general and domain-specific results\n",
    "merged = tf.keras.layers.Concatenate()([inp_gen, inp_spec])\n",
    "merged = tf.keras.layers.Dense(300, activation='sigmoid')(merged)\n",
    "# drop out layer and dense layer\n",
    "merged = tf.keras.layers.Dropout(.5)(merged)\n",
    "merged = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "classifier4 = tf.keras.Model([inp_gen,inp_spec], merged)\n",
    "#classifier.summary()|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_137\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_291 (InputLayer)          [(None, 1, 300)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_292 (InputLayer)          [(None, 1, 300)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_287 (Bidirectiona (None, 1, 600)       1442400     input_291[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_288 (Bidirectiona (None, 1, 600)       1442400     input_292[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_310 (SeqSelf [(None, None, 600),  38465       bidirectional_287[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_312 (SeqSelf [(None, None, 600),  38465       bidirectional_288[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_137 (Concatenate)   (None, None, 1200)   0           seq_self_attention_310[0][0]     \n",
      "                                                                 seq_self_attention_312[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_137 (Dropout)           (None, None, 1200)   0           concatenate_137[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_137 (Dense)               (None, None, 1)      1201        dropout_137[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 2,962,931\n",
      "Trainable params: 2,962,931\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# initialize the proposed classifier\n",
    "\n",
    "INPUT_SIZE = 300\n",
    "LATENT_SIZE = 300\n",
    "\n",
    "# domain-general model parts\n",
    "inp_gen = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "inp_gen_att, attn_weights_gen = SeqSelfAttention(return_attention = True)(inp_gen)\n",
    "out_gen = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE), return_sequences=True))(inp_gen)\n",
    "out_gen_att, attn_weights_gen = SeqSelfAttention(return_attention = True)(out_gen)\n",
    "\n",
    "# domain-specific model parts\n",
    "inp_spec = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "inp_spec_att, attn_weights_spec = SeqSelfAttention(return_attention = True)(inp_spec)\n",
    "out_spec = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE), return_sequences=True))(inp_spec)\n",
    "out_spec_att, attn_weights_spec= SeqSelfAttention(return_attention = True)(out_spec)\n",
    "\n",
    "# concatenate domain-general and domain-specific results\n",
    "merged = tf.keras.layers.Concatenate()([out_gen_att, out_spec_att])\n",
    "\n",
    "# drop out layer and dense layer\n",
    "merged = tf.keras.layers.Dropout(.5)(merged)\n",
    "merged = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "classifier3 = tf.keras.Model([inp_gen,inp_spec], merged)\n",
    "classifier3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_136\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_289 (InputLayer)          [(None, 1, 300)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_290 (InputLayer)          [(None, 1, 300)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_285 (Bidirectiona (None, 1, 600)       1442400     input_289[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_286 (Bidirectiona (None, 1, 600)       1442400     input_290[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_307 (SeqSelf [(None, None, 600),  38465       bidirectional_285[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_308 (SeqSelf [(None, None, 600),  38465       bidirectional_286[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_136 (Concatenate)   (None, None, 1200)   0           seq_self_attention_307[0][0]     \n",
      "                                                                 seq_self_attention_308[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_136 (Dropout)           (None, None, 1200)   0           concatenate_136[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_136 (Dense)               (None, None, 1)      1201        dropout_136[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 2,962,931\n",
      "Trainable params: 2,962,931\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# initialize the proposed classifier\n",
    "\n",
    "INPUT_SIZE = 300\n",
    "LATENT_SIZE = 300\n",
    "\n",
    "# domain-general model parts\n",
    "inp_gen = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "#inp_gen_att, attn_weights_gen = SeqSelfAttention(return_attention = True)(inp_gen)\n",
    "out_gen = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE), return_sequences=True))(inp_gen)\n",
    "out_gen_att, attn_weights_gen = SeqSelfAttention(return_attention = True)(out_gen)\n",
    "\n",
    "# domain-specific model parts\n",
    "inp_spec = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "#inp_spec_att, attn_weights_spec = SeqSelfAttention(return_attention = True)(inp_spec)\n",
    "out_spec = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE), return_sequences=True))(inp_spec)\n",
    "out_spec_att, attn_weights_spec= SeqSelfAttention(return_attention = True)(out_spec)\n",
    "\n",
    "# concatenate domain-general and domain-specific results\n",
    "merged = tf.keras.layers.Concatenate()([out_gen_att, out_spec_att])\n",
    "\n",
    "# drop out layer and dense layer\n",
    "merged = tf.keras.layers.Dropout(.5)(merged)\n",
    "merged = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "classifier2 = tf.keras.Model([inp_gen,inp_spec], merged)\n",
    "#classifier4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of the model\n",
    "plot_model(classifier, to_file='classifier_plot.png', show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "132/132 [==============================] - 1s 4ms/step - loss: 0.8106 - accuracy: 0.4995 - val_loss: 0.7297 - val_accuracy: 0.4700\n",
      "Epoch 2/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7108 - accuracy: 0.4948 - val_loss: 0.7047 - val_accuracy: 0.4700\n",
      "Epoch 3/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7142 - accuracy: 0.5029 - val_loss: 0.6918 - val_accuracy: 0.5300\n",
      "Epoch 4/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7096 - accuracy: 0.5031 - val_loss: 0.7511 - val_accuracy: 0.4700\n",
      "Epoch 5/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7142 - accuracy: 0.4945 - val_loss: 0.7018 - val_accuracy: 0.4700\n",
      "Epoch 6/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7220 - accuracy: 0.5074 - val_loss: 0.6969 - val_accuracy: 0.4700\n",
      "Epoch 7/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7106 - accuracy: 0.5107 - val_loss: 0.7025 - val_accuracy: 0.4700\n",
      "Epoch 8/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7183 - accuracy: 0.5112 - val_loss: 0.6904 - val_accuracy: 0.5667\n",
      "Epoch 9/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7217 - accuracy: 0.4936 - val_loss: 0.6965 - val_accuracy: 0.5300\n",
      "Epoch 10/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7229 - accuracy: 0.4988 - val_loss: 0.7110 - val_accuracy: 0.4700\n",
      "Epoch 11/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7124 - accuracy: 0.4907 - val_loss: 0.6912 - val_accuracy: 0.5300\n",
      "Epoch 12/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7140 - accuracy: 0.4983 - val_loss: 0.6924 - val_accuracy: 0.5300\n",
      "Epoch 13/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7203 - accuracy: 0.5129 - val_loss: 0.6925 - val_accuracy: 0.5017\n",
      "Epoch 14/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7214 - accuracy: 0.5019 - val_loss: 0.7376 - val_accuracy: 0.5300\n",
      "Epoch 15/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7212 - accuracy: 0.5014 - val_loss: 0.7376 - val_accuracy: 0.4700\n",
      "Epoch 16/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7217 - accuracy: 0.4943 - val_loss: 0.6929 - val_accuracy: 0.4700\n",
      "Epoch 17/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7115 - accuracy: 0.4971 - val_loss: 0.6916 - val_accuracy: 0.5300\n",
      "Epoch 18/30\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.7179 - accuracy: 0.4890 - val_loss: 0.7221 - val_accuracy: 0.4700\n"
     ]
    }
   ],
   "source": [
    "classifier4.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.01), metrics=['accuracy'])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/classifier/classifier_without_al/certainty_sampling/classifier_domain_data4_4+.h5\")\n",
    "    #print(X_train_gen.shape, X_train_spec.shape)\n",
    "history = classifier4.fit([np.expand_dims(np.asarray(X_train_gen).astype(np.float32), 1), np.expand_dims(np.asarray(X_train_spec).astype(np.float32), 1)], np.asarray(y_train).astype(np.float32), epochs=30, validation_data = ([np.expand_dims(np.asarray(X_val_gen).astype(np.float32), 1), np.expand_dims(np.asarray(X_val_spec).astype(np.float32), 1)], np.asarray(y_val).astype(np.float32)), callbacks = [checkpoint, es], batch_size=32)\n",
    "\n",
    "# evaluating the model\n",
    "score = classifier4.evaluate([np.expand_dims(np.asarray(X_test_gen).astype(np.float32), 1), np.expand_dims(np.asarray(X_test_spec).astype(np.float32), 1)], np.asarray(y_test).astype(np.float32), verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 300)"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_gen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline classifier (using specific embeddings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the baseline classifier\n",
    "\n",
    "INPUT_SIZE = 300\n",
    "LATENT_SIZE = 300\n",
    "\n",
    "classifier_baseline = tf.keras.Sequential()\n",
    "classifier_baseline.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(INPUT_SIZE, input_shape=(None,1,LATENT_SIZE))))\n",
    "classifier_baseline.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_baseline.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/classifier/classifier_without_al/baseline_specific/classifier_domain_0.h5\")\n",
    "history = classifier_baseline.fit(np.expand_dims(X_train_spec, 1), y_train, epochs=100,validation_data = (np.expand_dims(X_val_spec, 1), y_val),callbacks = [checkpoint,es],batch_size=32)\n",
    "\n",
    "\n",
    "# evaluating the model\n",
    "score = classifier_baseline.evaluate(np.expand_dims(X_test_spec, 1), y_test, verbose=0) \n",
    "print('Final accuracy score: '+str(score[1]))                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline classifier (using general embeddings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the baseline classifier\n",
    "\n",
    "INPUT_SIZE = 300\n",
    "LATENT_SIZE = 300\n",
    "\n",
    "classifier_baseline = Sequential()\n",
    "classifier_baseline.add(Bidirectional(LSTM(INPUT_SIZE, input_shape=(None,1,LATENT_SIZE))))\n",
    "classifier_baseline.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original, unsorted data\n",
    "with open('data/sentence_embeddings/general/unsorted/sentemb/sentemb.p', 'rb') as f:\n",
    "    data_general = pkl.load(f)\n",
    "\n",
    "with open('data/sentence_embeddings/general/unsorted/label_domain/label_domain_train_sentemb.p', 'rb') as f:\n",
    "    temp_train = pkl.load(f)\n",
    "\n",
    "with open('data/sentence_embeddings/general/unsorted/label_domain/label_domain_test_sentemb.p', 'rb') as f:\n",
    "    temp_test = pkl.load(f)\n",
    "    \n",
    "labels_general = np.hstack((temp_train, temp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices to reverse the shuffling that was done before the autoencoder\n",
    "# this is necessary to keep the sample order the exact same to the other baseline\n",
    "idx_train = np.random.RandomState(seed=42).permutation(25358)\n",
    "idx_train_new = np.argsort(idx_train)\n",
    "\n",
    "idx_test = np.random.RandomState(seed=43).permutation(6395)\n",
    "idx_test_new = np.argsort(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the shuffling that was done to the general embeddings before the autoencoder and shuffle them the same way as the specific embeddings\n",
    "# this is necessary in order to sustain the order\n",
    "data_train, data_test = data_general[:temp_train.shape[1]], data_general[temp_train.shape[1]:]\n",
    "temp_train, temp_test, data_train, data_test = temp_train[:,idx_train_new], temp_test[:,idx_test_new], data_train[idx_train_new], data_test[idx_test_new]\n",
    "temp_train, temp_test,data_train, data_test = temp_train[:,idx_train], temp_test[:,idx_test], data_train[idx_train], data_test[idx_test]\n",
    "\n",
    "index_to_keep = [index for index, value in enumerate(temp_train[1]) if int(value) == index_spec]\n",
    "temp_train, data_train = temp_train[:, index_to_keep], data_train[index_to_keep]\n",
    "\n",
    "index_to_keep = [index for index, value in enumerate(temp_test[1]) if int(value) == index_spec]\n",
    "temp_test, data_test = temp_test[:, index_to_keep], data_test[index_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data splitting\n",
    "X_spec = np.concatenate([data_train,data_test])\n",
    "X_train_spec, X_val_spec, X_test_spec = X_spec[:1400], X_spec[1400:1600], X_spec[1600:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_baseline.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint(filepath=\"weights/classifier/classifier_without_al/baseline_general/classifier_domain_0.h5\")\n",
    "history = classifier_baseline.fit(np.expand_dims(X_train_spec, 1), y_train, epochs=50,validation_data = (np.expand_dims(X_val_spec, 1), y_val),callbacks = [checkpoint,es],batch_size=32)\n",
    "\n",
    "\n",
    "# evaluating the model\n",
    "score = classifier_baseline.evaluate(np.expand_dims(X_test_spec, 1), y_test, verbose=0) \n",
    "print('Final accuracy score: '+str(score[1]))                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, choose one of the three classifiers to train (Model using general embeddings instead of specific embeddings, Model using BERT embeddings, Model choosing general embeddings out of the most similar domains to the target domain):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model using general embeddings instead of specific embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the proposed classifier\n",
    "\n",
    "INPUT_SIZE = 300\n",
    "LATENT_SIZE = 300\n",
    "\n",
    "# domain-general model parts\n",
    "inp_gen = Input(shape=(1,INPUT_SIZE))\n",
    "inp_gen_att, attn_weights_gen = SeqSelfAttention(return_attention = True)(inp_gen)\n",
    "out_gen = Bidirectional(LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_gen_att)\n",
    "\n",
    "# domain-specific model parts\n",
    "inp_spec = Input(shape=(1,INPUT_SIZE))\n",
    "inp_spec_att, attn_weights_spec = SeqSelfAttention(return_attention = True)(inp_spec)\n",
    "out_spec = Bidirectional(LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_spec_att)\n",
    "\n",
    "# concatenate domain-general and domain-specific results\n",
    "merged = Concatenate()([out_gen, out_spec])\n",
    "\n",
    "# drop out layer and dense layer\n",
    "merged = Dropout(.5)(merged)\n",
    "merged = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "classifier = Model([inp_gen,inp_spec], merged)\n",
    "#classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original, unsorted data\n",
    "with open('data/sentence_embeddings/general/unsorted/sentemb/sentemb.p', 'rb') as f:\n",
    "    data_general = pkl.load(f)\n",
    "\n",
    "with open('data/sentence_embeddings/general/unsorted/label_domain/label_domain_train_sentemb.p', 'rb') as f:\n",
    "    temp_train = pkl.load(f)\n",
    "\n",
    "with open('data/sentence_embeddings/general/unsorted/label_domain/label_domain_test_sentemb.p', 'rb') as f:\n",
    "    temp_test = pkl.load(f)\n",
    "    \n",
    "labels_general = np.hstack((temp_train, temp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices to reverse the shuffling that was done to the embeddings before the autoencoder\n",
    "# this is necessary in order to ensure that the order of the samples of the new input \n",
    "# is equivalent to the order of the sentence embeddings of the original model (specific sentence embeddings)\n",
    "idx_train = np.random.RandomState(seed=42).permutation(25358)\n",
    "idx_train_new = np.argsort(idx_train)\n",
    "\n",
    "idx_test = np.random.RandomState(seed=43).permutation(6395)\n",
    "idx_test_new = np.argsort(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the shuffling that was done to the general embeddings before the autoencoder and shuffle them the same way as the specific embeddings\n",
    "# this is necessary in order to sustain the original order\n",
    "data_train, data_test = data_general[:temp_train.shape[1]], data_general[temp_train.shape[1]:]\n",
    "temp_train, temp_test, data_train, data_test = temp_train[:,idx_train_new], temp_test[:,idx_test_new], data_train[idx_train_new], data_test[idx_test_new]\n",
    "temp_train, temp_test,data_train, data_test = temp_train[:,idx_train], temp_test[:,idx_test], data_train[idx_train], data_test[idx_test]\n",
    "\n",
    "index_to_keep = [index for index, value in enumerate(temp_train[1]) if int(value) == index_spec]\n",
    "temp_train, data_train = temp_train[:, index_to_keep], data_train[index_to_keep]\n",
    "\n",
    "index_to_keep = [index for index, value in enumerate(temp_test[1]) if int(value) == index_spec]\n",
    "temp_test, data_test = temp_test[:, index_to_keep], data_test[index_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data splitting\n",
    "X_spec = np.concatenate([data_train,data_test])\n",
    "X_train_spec, X_val_spec, X_test_spec = X_spec[:1400], X_spec[1400:1600], X_spec[1600:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint(filepath=\"weights/classifier/classifier_without_al/only_general_embeddings/classifier_domain_0.h5\")\n",
    "history = classifier.fit([np.expand_dims(X_train_gen, 1), np.expand_dims(X_train_spec, 1)], y_train, epochs=50, validation_data = ([np.expand_dims(X_val_gen, 1), np.expand_dims(X_val_spec, 1)], y_val), callbacks = [checkpoint, es], batch_size=32)\n",
    "\n",
    "# evaluating the model\n",
    "score = classifier.evaluate([np.expand_dims(X_test_gen, 1), np.expand_dims(X_test_spec, 1)], y_test, verbose=0) \n",
    "print('Final accuracy score: '+str(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model using BERT embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for BERT embeddings\n",
    "\n",
    "INPUT_SIZE_BERT = 768\n",
    "INPUT_SIZE_SPEC = 300\n",
    "LATENT_SIZE = 300\n",
    "\n",
    "# domain-general model parts\n",
    "input_gen = Input(shape=(1,INPUT_SIZE_BERT))\n",
    "input_att_gen, attn_weights_gen = SeqSelfAttention(return_attention = True)(input_gen)\n",
    "output_gen = Bidirectional(LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE_BERT)))(input_att_gen)\n",
    "\n",
    "# domain-specific model parts\n",
    "input_spec = Input(shape=(1,INPUT_SIZE_SPEC))\n",
    "input_att_spec, attn_weights_spec = SeqSelfAttention(return_attention = True)(input_spec)\n",
    "output_spec = Bidirectional(LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE_SPEC)))(input_att_spec)\n",
    "\n",
    "# concatenate domain-general and domain-specific results\n",
    "merged = Concatenate()([output_gen, output_spec])\n",
    "\n",
    "# drop out layer and dense layer\n",
    "merged = Dropout(.5)(merged)\n",
    "merged = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "classifier_bert_embeddings = Model([input_gen,input_spec], merged)\n",
    "#classifier_bert_embeddings.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare the data\n",
    "df_train, df_test = load_data('data/bert_embeddings/train/*'), load_data('data/bert_embeddings/test/*')\n",
    "df_train['domain'], df_test['domain'] = [re.sub('.task.train.pkl$', '', word) for word in np.array(df_train['domain'])], [re.sub('.task.test.pkl$', '', word) for word in np.array(df_test['domain'])]\n",
    "\n",
    "# create a dictionary that numerically encodes the domain\n",
    "dict_domain = dict(zip(sorted(set(np.array(df_train['domain']))), np.arange(16)))\n",
    "\n",
    "# divide the data in embeddings and encodings of label/domain\n",
    "X_train, label_domain_train = divide_data(df_train, dict_domain)\n",
    "X_test, label_domain_test = divide_data(df_test, dict_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data (same shuffling as sentence embeddings generated by the autoencoder)\n",
    "idx_train = np.random.RandomState(seed=42).permutation(X_train.shape[0])\n",
    "X_train,label_domain_train = X_train[idx_train], label_domain_train[:,idx_train]\n",
    "\n",
    "idx_test = np.random.RandomState(seed=43).permutation(X_test.shape[0])\n",
    "X_test,label_domain_test = X_test[idx_test], label_domain_test[:,idx_test]\n",
    "\n",
    "labels_general, data_general = np.hstack((label_domain_train, label_domain_test)), np.hstack((X_train, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices for sorting the array\n",
    "ind = sort_array(labels_general, labels_total)\n",
    "\n",
    "# sort general sentence embeddings\n",
    "data_general, labels_general = data_general[ind], labels_general[:, ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the indices of all samples that aren't nan in order to eliminate them in both general and specific embeddings to sustain the input pairs\n",
    "indices_to_remove = []\n",
    "for i in range(data_general.shape[0]):\n",
    "    if np.isnan(data_general[i]).any():\n",
    "        indices_to_remove.append(i)\n",
    "        \n",
    "indices_to_keep = list(set(np.arange(0,data_general.shape[0])) - set(indices_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "data_general, labels_general, X_spec = data_general[indices_to_keep], labels_general[:, indices_to_keep], X_spec[indices_to_keep]\n",
    "    \n",
    "X_train_spec, X_val_spec, X_test_spec = X_spec[:1400], X_spec[1400:1600], X_spec[1600:] \n",
    "y_train, y_valid, y_test = labels_general[0, :1400], labels_general[0, 1400:1600], labels_general[0, 1600:]\n",
    "X_train_gen, X_val_gen, X_test_gen = data_general[:1400], data_general[1400:1600], data_general[1600:]\n",
    "X_train_gen, X_val_gen, X_test_gen = np.vstack(X_train_gen), np.vstack(X_val_gen), np.vstack(X_test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "classifier_bert_embeddings.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint(filepath=\"weights/classifier/classifier_without_al/bert_embedding_usage/classifier_domain_0.h5\")\n",
    "history = classifier_bert_embeddings.fit([np.expand_dims(X_train_gen, 1), np.expand_dims(X_train_spec, 1)], y_train, epochs=50, validation_data = ([np.expand_dims(X_val_gen, 1), np.expand_dims(X_val_spec, 1)], y_val), callbacks = [checkpoint, es], batch_size=32)\n",
    "\n",
    "# evaluating the model\n",
    "score = classifier_bert_embeddings.evaluate([np.expand_dims(X_test_gen, 1), np.expand_dims(X_test_spec, 1)], y_test, verbose=0) \n",
    "print('Final accuracy score: '+str(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model choosing general embeddings out of the most similar domains to the target domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-746-a69000415596>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# domain-general model parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0minp_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mINPUT_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0minp_gen_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeqSelfAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mout_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLATENT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mINPUT_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_gen_att\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize the proposed classifier\n",
    "\n",
    "INPUT_SIZE = 300\n",
    "LATENT_SIZE = 300\n",
    "\n",
    "# domain-general model parts\n",
    "inp_gen = Input(shape=(1,INPUT_SIZE))\n",
    "inp_gen_att, attn_weights_gen = SeqSelfAttention(return_attention = True)(inp_gen)\n",
    "out_gen = Bidirectional(LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_gen_att)\n",
    "\n",
    "# domain-specific model parts\n",
    "inp_spec = Input(shape=(1,INPUT_SIZE))\n",
    "inp_spec_att, attn_weights_spec = SeqSelfAttention(return_attention = True)(inp_spec)\n",
    "out_spec = Bidirectional(LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_spec_att)\n",
    "\n",
    "# concatenate domain-general and domain-specific results\n",
    "merged = Concatenate()([out_gen, out_spec])\n",
    "\n",
    "# drop out layer and dense layer\n",
    "merged = Dropout(.5)(merged)\n",
    "merged = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "classifier = Model([inp_gen,inp_spec], merged)\n",
    "#classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original, unsorted data\n",
    "with open('data/sentence_embeddings/general/unsorted/sentemb/sentemb_unlabeled3.p', 'rb') as f:\n",
    "    data_general = pkl.load(f)\n",
    "\n",
    "with open('data/sentence_embeddings/general/unsorted/label_domain/label_domain_train_sentemb_unlabeled3.p', 'rb') as f:\n",
    "    labels_train = pkl.load(f)\n",
    "    \n",
    "with open('data/sentence_embeddings/general/unsorted/label_domain/label_domain_test_sentemb_unlabeled3.p', 'rb') as f:\n",
    "    labels_test = pkl.load(f)\n",
    "    \n",
    "labels_general = np.hstack((labels_train, labels_test))\n",
    "\n",
    "data_general = data_general.transpose()\n",
    "\n",
    "# load the cleaned data\n",
    "with open('data/cleaned_data/merged_cleaned.p', 'rb') as f:\n",
    "    df_train = pkl.load(f)\n",
    "with open('data/cleaned_data/test_cleaned.p', 'rb') as f:\n",
    "    df_test = pkl.load(f)\n",
    "\n",
    "# create a list of data frames dfs, each data frame represents one domain\n",
    "#df = pd.concat([df_train, df_test],ignore_index=True)\n",
    "#dfs = [x for _, x in df.groupby('domain')]\n",
    "\n",
    "list_unlabel = df_train.index[df_train['label'] == 3].to_list()\n",
    "\n",
    "df_train = df_train[~df_train.index.isin(list_unlabel)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# create a list of data frames dfs, each data frame represents one domain\n",
    "df = pd.concat([df_train, df_test],ignore_index=True)\n",
    "dfs = [x for _, x in df.groupby('domain')]\n",
    "\n",
    "# get the word distribution of each domain\n",
    "# the frequency of each existing word is computed in every domain\n",
    "import collections\n",
    "import regex as re\n",
    "word_counter = []\n",
    "for df in dfs:\n",
    "    counts = collections.Counter()\n",
    "    words = re.compile(r'\\w+')\n",
    "    reviews = np.array([s for s in df['text']])\n",
    "    for review in reviews:\n",
    "        counts.update(words.findall(review.lower()))\n",
    "    word_counter.append(counts)\n",
    "\n",
    "# the rows of df are the 16 domains, the columns are all existing words\n",
    "# the number of the cells of df is the word frequency for the word in the domain\n",
    "df_dist = pd.DataFrame(word_counter)\n",
    "df_dist = df_dist.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    0    0    0]\n",
      " [  14   14    7 ...    9   13    2]\n",
      " [1246  852  405 ...  552 1349 1230]] [[0 0 0 ... 0 0 0]\n",
      " [4 10 15 ... 11 11 11]\n",
      " [96 916 1208 ... 352 1303 490]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "# get list js_d of jensen_shannon distances to the target domain\n",
    "js_d = []\n",
    "for i in range(df_dist.shape[0]):\n",
    "    d = distance.jensenshannon(np.array(df_dist.iloc[index_spec]), np.array(df_dist.iloc[i]))\n",
    "    js_d.append(d)\n",
    "    \n",
    "# take 5 most similiar distributions\n",
    "# most_sim_dist is a list of 5 elements with the 5 closest domains to the target domain\n",
    "most_sim_dist = sorted(range(len(js_d)), key=lambda i: js_d[i], reverse=True)[-6:]\n",
    "most_sim_dist.remove(index_spec)\n",
    "#print(labels_general.shape)\n",
    "# remove general embeddings that aren't from these 5 domains\n",
    "index_to_keep = [index for index, value in enumerate(labels_general[1]) if int(value) in most_sim_dist]\n",
    "labels_general, data_general = labels_general[:, index_to_keep], data_general[index_to_keep]\n",
    "#print(labels_general.shape)\n",
    "# function for sorting two arrays such that both arrays have the same labels\n",
    "# returns indeces_sorted which consists of indices and is used for sorting array_to_sort\n",
    "def sort_array(array_to_sort, array_ref):\n",
    "    \n",
    "    y, y_ref = array_to_sort[0].astype(int), array_ref[0].astype(int)\n",
    "    indeces_zeros, indeces_ones = [], []\n",
    "\n",
    "    # get indices when array_to_sort is 0 (indeces_zeros) and when it is 1 (indeces_ones)\n",
    "    for i in np.arange(y.shape[0]):\n",
    "        if y[i] == 0:\n",
    "            indeces_zeros.append(i)\n",
    "        else:\n",
    "            indeces_ones.append(i)\n",
    "\n",
    "    indeces_sorted = np.zeros(y_ref.shape[0])\n",
    "    cnt_zeros, cnt_ones = 0,0\n",
    "    \n",
    "    # get sorted indeces\n",
    "    # pair the first positive (/negative) instance of both arrays, etc. \n",
    "    for i in np.arange(y_ref.shape[0]):\n",
    "        if y_ref[i] == 0:\n",
    "            indeces_sorted[i] = indeces_zeros[cnt_zeros]\n",
    "            cnt_zeros += 1\n",
    "        else:\n",
    "            indeces_sorted[i] = indeces_ones[cnt_ones]\n",
    "            cnt_ones += 1\n",
    "    \n",
    "    return indeces_sorted.astype(int)\n",
    "#print(labels_general.shape, labels_total.shape)\n",
    "# get indices for sorting the array\n",
    "print(labels_general, labels_total)\n",
    "ind = sort_array(labels_general, labels_total)\n",
    "\n",
    "# sort general sentence embeddings\n",
    "data_general, labels_general = data_general[ind], labels_general[:, ind]\n",
    "\n",
    "# data splitting\n",
    "X_train_gen, X_val_gen, X_test_gen = data_general[:4200], data_general[4200:4800], data_general[4800:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1560,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2088,
   "metadata": {},
   "outputs": [],
   "source": [
    " class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        INPUT_SIZE = 300\n",
    "        LATENT_SIZE = 300\n",
    "   # hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    \n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "        hp_units1 = hp.Int('units1', min_value=100, max_value=500, step=100)\n",
    "       # hp_units2 = hp.Int('units2', min_value=60, max_value=130, step=10)\n",
    "# domain-general model parts\n",
    "        inp_gen = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "      #  inp_gen_att, attn_weights_gen = SeqSelfAttention(return_attention = True)(inp_gen)\n",
    "        #out_gen = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hp_units1, input_shape=(None,1,INPUT_SIZE)))(inp_gen)\n",
    "        #out_gen = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hp_units2, input_shape=(None,1,INPUT_SIZE)))(out_gen1)\n",
    "# domain-specific model parts\n",
    "        inp_spec = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "       # inp_spec_att, attn_weights_spec = SeqSelfAttention(return_attention = True)(inp_spec)\n",
    "        #out_spec = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300, input_shape=(None,1,INPUT_SIZE)))(inp_spec)\n",
    "       # out_spec = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(120, input_shape=(None,1,INPUT_SIZE)))(out_spec1)\n",
    "# concatenate domain-general and domain-specific results\n",
    "        merged = tf.keras.layers.Concatenate()([inp_gen, inp_spec])\n",
    "        merged = tf.keras.layers.Dense(hp_units1, activation='sigmoid')(merged)\n",
    "# drop out layer and dense layer\n",
    "        merged = tf.keras.layers.Dropout(hp.Float('dropout', 0, 0.5, step=0.1, default=0.5))(merged)\n",
    "        merged = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "        classifier = tf.keras.Model([inp_gen,inp_spec], merged)\n",
    "        classifier.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), metrics=['accuracy'])\n",
    "#classifier.summary()\n",
    "        return classifier\n",
    "        \n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", values=[32, 64]),\n",
    "            epochs= hp.Int('epochs', min_value=20, max_value=70, step=10),\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2089,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner2=kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=50,\n",
    "    overwrite=True,\n",
    "    num_initial_points=25,\n",
    "    alpha=0.001,\n",
    "    beta=2.6\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2090,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 42 Complete [00h 00m 07s]\n",
      "val_accuracy: 0.8833333253860474\n",
      "\n",
      "Best val_accuracy So Far: 0.8899999856948853\n",
      "Total elapsed time: 00h 08m 46s\n",
      "\n",
      "Search: Running Trial #43\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.01              |0.01              |learning_rate\n",
      "100               |300               |units1\n",
      "0.5               |0.3               |dropout\n",
      "64                |32                |batch_size\n",
      "70                |40                |epochs\n",
      "\n",
      "Epoch 1/70\n",
      "66/66 [==============================] - 1s 5ms/step - loss: 0.6074 - accuracy: 0.6595 - val_loss: 0.4791 - val_accuracy: 0.7850\n",
      "Epoch 2/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.4326 - accuracy: 0.8045 - val_loss: 0.3633 - val_accuracy: 0.8533\n",
      "Epoch 3/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.3633 - accuracy: 0.8395 - val_loss: 0.3510 - val_accuracy: 0.8500\n",
      "Epoch 4/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.3471 - accuracy: 0.8488 - val_loss: 0.3480 - val_accuracy: 0.8450\n",
      "Epoch 5/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.3184 - accuracy: 0.8629 - val_loss: 0.3210 - val_accuracy: 0.8600\n",
      "Epoch 6/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.3053 - accuracy: 0.8705 - val_loss: 0.3215 - val_accuracy: 0.8700\n",
      "Epoch 7/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2996 - accuracy: 0.8726 - val_loss: 0.3200 - val_accuracy: 0.8717\n",
      "Epoch 8/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2866 - accuracy: 0.8783 - val_loss: 0.3155 - val_accuracy: 0.8667\n",
      "Epoch 9/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2899 - accuracy: 0.8767 - val_loss: 0.3120 - val_accuracy: 0.8667\n",
      "Epoch 10/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2868 - accuracy: 0.8833 - val_loss: 0.3147 - val_accuracy: 0.8617\n",
      "Epoch 11/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2826 - accuracy: 0.8812 - val_loss: 0.3073 - val_accuracy: 0.8733\n",
      "Epoch 12/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2776 - accuracy: 0.8793 - val_loss: 0.3259 - val_accuracy: 0.8550\n",
      "Epoch 13/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2696 - accuracy: 0.8907 - val_loss: 0.3098 - val_accuracy: 0.8600\n",
      "Epoch 14/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2633 - accuracy: 0.8881 - val_loss: 0.3101 - val_accuracy: 0.8683\n",
      "Epoch 15/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2633 - accuracy: 0.8931 - val_loss: 0.3062 - val_accuracy: 0.8700\n",
      "Epoch 16/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2637 - accuracy: 0.8943 - val_loss: 0.3104 - val_accuracy: 0.8783\n",
      "Epoch 17/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2557 - accuracy: 0.8910 - val_loss: 0.3083 - val_accuracy: 0.8667\n",
      "Epoch 18/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2587 - accuracy: 0.8933 - val_loss: 0.3104 - val_accuracy: 0.8817\n",
      "Epoch 19/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2534 - accuracy: 0.8912 - val_loss: 0.3080 - val_accuracy: 0.8783\n",
      "Epoch 20/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2513 - accuracy: 0.8933 - val_loss: 0.3049 - val_accuracy: 0.8700\n",
      "Epoch 21/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2446 - accuracy: 0.8971 - val_loss: 0.3315 - val_accuracy: 0.8750\n",
      "Epoch 22/70\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.2520 - accuracy: 0.8962 - val_loss: 0.3187 - val_accuracy: 0.8533\n",
      "Epoch 23/70\n",
      "25/66 [==========>...................] - ETA: 0s - loss: 0.2544 - accuracy: 0.8900"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2090-7a43e554a23e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#checkpoint = ModelCheckpoint(filepath=\"weights/classifier/classifier_without_al/standard_model/classifier_domain_1.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtuner2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;31m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         return tuner_utils.convert_to_metrics_dict(\n\u001b[1;32m    224\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HyperModel.fit()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2088-60b21c0a06f9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m            \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m            \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m        )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \"\"\"\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1099\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#NUM_EPOCHS = 20\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "#checkpoint = ModelCheckpoint(filepath=\"weights/classifier/classifier_without_al/standard_model/classifier_domain_1.h5\")\n",
    "\n",
    "tuner2.search([np.expand_dims(X_train_gen, 1), np.expand_dims(X_train_spec, 1)], y_train, validation_data = ([np.expand_dims(X_val_gen, 1), np.expand_dims(X_val_spec, 1)], y_val), callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner2.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal batch_size\n",
    "layer is {best_hps.get('batch_size')}, the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}, the optimal dropout rate is {best_hps.get('dropout')}, the optimal number of epochs is {best_hps.get('epochs')} the optimal number of units1 is {best_hps.get('units1')} and th.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#hp_units1 = hp.Int('units', min_value=200, max_value=350, step=10)\n",
    "#hp_units2 = hp.Int('units', min_value=50, max_value=180, step=10)\n",
    "# domain-general model parts\n",
    "inp_gen = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "#inp_gen_att, attn_weights_gen = SeqSelfAttention(return_attention = True)(inp_gen)\n",
    "#out_gen = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300, input_shape=(None,1,INPUT_SIZE)))(inp_gen)\n",
    "#out_gen = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(120, input_shape=(None,1,INPUT_SIZE)))(out_gen1)\n",
    "\n",
    "\n",
    "# domain-specific model parts\n",
    "inp_spec = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "#inp_spec_att, attn_weights_spec = SeqSelfAttention(return_attention = True)(inp_spec)\n",
    "#out_spec = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300, input_shape=(None,1,INPUT_SIZE)))(inp_spec)\n",
    "#out_spec = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(120, input_shape=(None,1,INPUT_SIZE)))(out_spec1)\n",
    "\n",
    "\n",
    "# concatenate domain-general and domain-specific results\n",
    "merged = tf.keras.layers.Concatenate()([inp_gen, inp_spec])\n",
    "#merged = tf.keras.layers.AveragePooling1D()([out_gen, out_spec])\n",
    "merged = tf.keras.layers.Dense(100, activation='sigmoid')(merged)\n",
    "# drop out layer and dense layer\n",
    "#merged = tf.keras.layers.Dense(300, activation='sigmoid')(merged)\n",
    "merged = tf.keras.layers.Dropout(0.3)(merged)\n",
    "merged = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "classifier = tf.keras.Model([inp_gen,inp_spec], merged)\n",
    "classifier.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 300) (4200, 300) (4200,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2134-f5014fb4bd81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weights/classifier/classifier_without_al/standard_model/classifier_domain_3_15.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# evaluating the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m                **kwargs):\n\u001b[1;32m    229\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    232\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_is_scipy_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1429\u001b[0m   \"\"\"\n\u001b[1;32m   1430\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1431\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1439\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \"\"\"\n\u001b[1;32m    271\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 272\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "print(X_train_gen.shape, X_train_spec.shape, y_train.shape)\n",
    "classifier.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/classifier/classifier_without_al/standard_model/classifier_domain_3_15.h5\")\n",
    "history = classifier.fit([np.expand_dims(X_train_gen, 1), np.expand_dims(X_train_spec, 1)], y_train, epochs=30, validation_data = ([np.expand_dims(X_val_gen, 1), np.expand_dims(X_val_spec, 1)], y_val), callbacks = [checkpoint, es], batch_size=64)\n",
    "\n",
    "# evaluating the model\n",
    "score = classifier.evaluate([np.expand_dims(X_test_gen, 1), np.expand_dims(X_test_spec, 1)], y_test, verbose=1) \n",
    "print('Final accuracy score: '+str(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
