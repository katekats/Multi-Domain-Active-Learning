{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import nltk\n",
    "import h5py\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Setup\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Adjust stopwords list\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "stopwords_keep = ['no', 'not', 'nor']\n",
    "stopwords_adjusted = list(stopwords_list.difference(stopwords_keep))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Function to clean a given text.\"\"\"\n",
    "    # Handle contracted words\n",
    "    contracted_words = {\n",
    "        'ca': 'can',\n",
    "        'wo': 'will',\n",
    "        'sha': 'shall',\n",
    "        'nt': \"not\",\n",
    "        \"n't\": \"not\"\n",
    "    }\n",
    "    text = ' '.join([contracted_words.get(word, word) for word in text.split()])\n",
    "    # Remove punctuation & special characters\n",
    "    text = ' '.join(re.split('\\W+', text))\n",
    "    text = ' '.join(word for word in text.split() if word.isalnum())\n",
    "    # Remove nouns and numbers\n",
    "    tagged_text = nltk.tag.pos_tag(text.split())\n",
    "    text = ' '.join([word for word, tag in tagged_text if tag != 'CD'])\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords_adjusted])\n",
    "    return text\n",
    "\n",
    "def load_and_clean_data(path, has_label=True):\n",
    "    \"\"\"Load data from the given path, clean it, and return a dataframe.\"\"\"\n",
    "    df_list = []\n",
    "    for file_name in glob.glob(path):\n",
    "        columns = [\"label\", \"text\"] if has_label else [\"text\"]\n",
    "        df_temp = pd.read_csv(file_name, delimiter='\\t', names=columns, encoding='latin-1')\n",
    "        df_temp['domain'] = os.path.basename(file_name)\n",
    "        df_list.append(df_temp)\n",
    "    \n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    # Delete empty strings\n",
    "    df['text'].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "import os\n",
    "\n",
    "def ensure_directory_exists(filepath):\n",
    "    \"\"\"Ensure the directory of the given filepath exists.\"\"\"\n",
    "    # Extract the directory path from the full filepath\n",
    "    directory = os.path.dirname(filepath)\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        # If not, create the directory (and any intermediate directories as needed)\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the entire data processing pipeline.\"\"\"\n",
    "    # Load and clean data\n",
    "    df_train = load_and_clean_data('data/uncleaned_data/train/*')\n",
    "    df_test = load_and_clean_data('data/uncleaned_data/test/*')\n",
    "    df_unlabel = load_and_clean_data('data/uncleaned_data/unlabelled/*', has_label=False)\n",
    "    \n",
    "    # Process domain names\n",
    "    domain_replacements = {\n",
    "        '.task.train$': '',\n",
    "        '.task.test$': '',\n",
    "        '.task.unlabel$': ''\n",
    "    }\n",
    "    for pattern, replacement in domain_replacements.items():\n",
    "        df_train['domain'] = df_train['domain'].str.replace(pattern, replacement)\n",
    "        df_test['domain'] = df_test['domain'].str.replace(pattern, replacement)\n",
    "        df_unlabel['domain'] = df_unlabel['domain'].str.replace(pattern, replacement)\n",
    "    \n",
    "    # Add label to unlabelled data\n",
    "    df_unlabel[\"label\"] = 3\n",
    "\n",
    "    # Merge datasets\n",
    "    df_merged = pd.concat([df_train, df_unlabel], ignore_index=True)\n",
    "\n",
    "   # Save data\n",
    "    save_paths = {\n",
    "        \"data/cleaned_data/train_cleaned.p\": df_train,\n",
    "        \"data/cleaned_data/test_cleaned.p\": df_test,\n",
    "        \"data/cleaned_data/merged_cleaned.p\": df_merged\n",
    "}\n",
    "\n",
    "    for path, dataframe in save_paths.items():\n",
    "        ensure_directory_exists(path)  # Make sure the directory exists\n",
    "        with open(path, \"wb\") as file:\n",
    "            pkl.dump(dataframe, file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
