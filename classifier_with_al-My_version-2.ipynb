{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file produces all the active learning results of the evaluation chapter. All cells needs to be executed until the headline \"Executing Active Learning and training the model\" is reached. Then, a specific model is chosen and executed. Note: Make sure to select the desired target domain when loading the data below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to adjust the checkpoint paths when training the models such that the weights are saved in the desired paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and setting configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target domain\n",
    "index_spec = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_self_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_179/1087900879.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#from keras.callbacks import ModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#from keras.layers.wrappers import Bidirectional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_self_attention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeqSelfAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#from keras.layers import Input, Dense, Concatenate, Dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_self_attention'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# it's possible that a deprecation warning appears here for unmath_test, however unrelevant for this work\n",
    "#from keras.models import Model\n",
    "#from keras.layers.recurrent import LSTM\n",
    "#from keras.callbacks import EarlyStopping\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "#from keras.layers.wrappers import Bidirectional\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "#from keras.layers import Input, Dense, Concatenate, Dropout\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy as np1\n",
    "import random as rn\n",
    "import pickle as pkl\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "    \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seeds in order to reproduce the results\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(1)\n",
    "rn.seed(2)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "\n",
    "# configurations so we use a single thread\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The necessary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to load the data of the desired target domain here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for sorting two arrays such that both arrays have the same labels\n",
    "def sort_array(labels, labels_ref):\n",
    "    index_gen_zeros = []\n",
    "    index_gen_ones = []\n",
    "    \n",
    "    # get indices when the general label is 0 (index_gen_zeros) and when it is 1 (index_gen_ones)\n",
    "    for i in np.arange(len(list(labels))):\n",
    "        if int(labels[i]) == 0:\n",
    "            index_gen_zeros.append(i)\n",
    "        if int(labels[i]) == 1: \n",
    "            index_gen_ones.append(i)\n",
    "    cnt_0, cnt_1 = 0,0\n",
    "    result_ind = np.zeros(len(list(labels_ref)))\n",
    "    \n",
    "    # sort array such that the first positive specific embedding gets paired with the first positive general embedding\n",
    "    # as well as the first negative specific embeddings gets paired with the first negative general embedding\n",
    "    for i in np.arange(len(list(labels_ref))):\n",
    "        if int(labels_ref[i]) == 0:\n",
    "            result_ind[i] = int(index_gen_zeros[cnt_0])\n",
    "            cnt_0 = cnt_0+1\n",
    "        else:\n",
    "            result_ind[i] = int(index_gen_ones[cnt_1])\n",
    "            cnt_1 = cnt_1+1\n",
    "    \n",
    "    return result_ind.astype(int)\n",
    "\n",
    "def sort_array2(array_to_sort, array_ref):\n",
    "    \n",
    "    y, y_ref = array_to_sort[0].astype(int), array_ref[0].astype(int)\n",
    "    indeces_zeros, indeces_ones = [], []\n",
    "\n",
    "    # get indices when array_to_sort is 0 (indeces_zeros) and when it is 1 (indeces_ones)\n",
    "    for i in np.arange(y.shape[0]):\n",
    "        if y[i] == 0:\n",
    "            indeces_zeros.append(i)\n",
    "        else:\n",
    "            indeces_ones.append(i)\n",
    "\n",
    "    indeces_sorted = np.zeros(y_ref.shape[0])\n",
    "    cnt_zeros, cnt_ones = 0,0\n",
    "    \n",
    "    # get sorted indeces\n",
    "    # pair the first positive (/negative) instance of both arrays, etc. \n",
    "    for i in np.arange(y_ref.shape[0]):\n",
    "        if y_ref[i] == 0:\n",
    "            indeces_sorted[i] = indeces_zeros[cnt_zeros]\n",
    "            cnt_zeros += 1\n",
    "        else:\n",
    "            indeces_sorted[i] = indeces_ones[cnt_ones]\n",
    "            cnt_ones += 1\n",
    "    \n",
    "    return indeces_sorted.astype(int)\n",
    "\n",
    "# function for removing outliers\n",
    "# choose parameter \"algorithm\" as 0 for no outlier detection, 1 for elliptic envelope\n",
    "# and choose 2 for isolation forest\n",
    "def outlier_removal(X, y, algorithm):\n",
    "\n",
    "    # no outlier removal\n",
    "    if algorithm == 0:\n",
    "        outlier_removal = None\n",
    "        \n",
    "    # outlier removal\n",
    "    else: \n",
    "        if algorithm == 1:\n",
    "            outlier_removal = EllipticEnvelope(support_fraction=0.9, random_state = 2)\n",
    "        elif algorithm == 2:\n",
    "            outlier_removal = IsolationForest(random_state = 3)\n",
    "        print(X)\n",
    "        # fit and predict\n",
    "        outlier_removal.fit(X)\n",
    "        y_hat = outlier_removal.predict(X)\n",
    "\n",
    "        # select all rows that are not outliers\n",
    "        mask = y_hat != -1\n",
    "\n",
    "        # remove outliers\n",
    "        X, y = X[mask, :], y[mask]\n",
    "    \n",
    "\n",
    "    return X, y\n",
    "\n",
    "# function for active learning\n",
    "# parameter data and labels contain original train set\n",
    "# parameter X_test and y_test are original validation and test set stacked\n",
    "# max_query denotes the number of instances queried per iteration\n",
    "# uncertainty sampling binarily encodes uncertainty sampling or certainty sampling\n",
    "# outlier detection indicates the outlier detection technique\n",
    "def AL(data_gen, data_spec, labels_gen, labels_spec, X_val_gen, X_test_gen, X_val_spec, X_test_spec, y_val_gen, y_test_gen, y_val_spec, y_test_spec, max_query1, max_query2, uncertainty_sampling, outlier_detection):\n",
    "    \n",
    "    # remove outliers, choose outlier detection algorithm (parameter \"algo\")\n",
    "    data_gen, labels_gen = outlier_removal(data_gen, labels_gen, outlier_detection)\n",
    "    data_spec, labels_spec = outlier_removal(data_spec, labels_spec, outlier_detection)\n",
    "    # initializations\n",
    "    k = 100\n",
    "     \n",
    "    continue_al = True\n",
    "    X_train_gen, X_valid_gen, y_train_gen, y_valid_gen = data_gen[:150], data_gen[150:], labels_gen[:150], labels_gen[150:] \n",
    "    X_train_spec, X_valid_spec, y_train_spec, y_valid_spec = data_spec[:100], data_spec[100:], labels_spec[:100], labels_spec[100:] \n",
    "   \n",
    "    \n",
    "  \n",
    "    \n",
    "    #CLASSIFIER\n",
    "    \n",
    "    INPUT_SIZE = 300\n",
    "    LATENT_SIZE = 300\n",
    "\n",
    "# domain-general model parts\n",
    "    inp_gen = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "    inp_spec = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "    merged = tf.keras.layers.Concatenate()([inp_gen, inp_spec])\n",
    "    merged = tf.keras.layers.Dense(100, activation='sigmoid')(merged)\n",
    "   # merged = tf.keras.layers.Dense(100, activation='sigmoid')(merged)\n",
    "# drop out layer and dense layer\n",
    "    merged = tf.keras.layers.Dropout(.5)(merged)\n",
    "    merged = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    classifier = tf.keras.Model([inp_gen,inp_spec], merged)\n",
    "    #classifier.summary()\n",
    "     \n",
    "    ind4 = sort_array(y_test_gen, y_test_spec)\n",
    "    X_test_gen, y_test_gen = X_test_gen[ind4], y_test_spec\n",
    "    #print(X_test_gen1.shape, y_test_gen1.shape, y_test_spec.shape, y_test_spec.shape)\n",
    "    \n",
    "    ind5 = sort_array(y_val_gen, y_val_spec)\n",
    "    X_val_gen, y_val_gen = X_val_gen[ind5], y_val_spec\n",
    "    #print(X_val_gen1.shape, y_val_gen1.shape, X_val_spec.shape, y_val_spec.shape)\n",
    "    # apply AL on specific and general sentence embeddings\n",
    "# make sure to pick the right parameters for uncertainty_sampling and outlier_detection \n",
    "# SIDENOTE: for certainty sampling experiment change max_query to 2200 (for domain 4 even to 3000) for general embeddings - due to different number of positive/negative samples\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "    # start active learning loop, execute loops until stopping criteria is fulfilled\n",
    "    while continue_al:  \n",
    "              \n",
    "        \n",
    "       # print(y_train_gen.shape, y_train_spec.shape)\n",
    "        ind2 = sort_array(y_train_gen, y_train_spec)\n",
    "        X_train_gen1, y_train1 = X_train_gen[ind2], y_train_spec\n",
    "        #print(y_train1.shape)\n",
    "    #THE LSTM Classifier\n",
    "        #print(y_valid_gen.shape, y_valid_spec.shape)\n",
    "        ind3 = sort_array(y_valid_gen, y_valid_spec)\n",
    "        X_valid_gen1, y_valid_gen1 = X_valid_gen[ind3], y_valid_spec\n",
    "        #print(y_test_gen.shape, y_test_spec.shape)\n",
    "        \n",
    "        \n",
    "        # scale the data\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_train_gen1 = scaler.fit_transform(X_train_gen1)\n",
    "        X_valid_gen1 = scaler.transform(X_valid_gen1)\n",
    "        X_test_gen = scaler.transform(X_test_gen)\n",
    "        X_val_gen = scaler.transform(X_val_gen)\n",
    "        \n",
    "        X_train_spec = scaler.fit_transform(X_train_spec)\n",
    "        X_valid_spec = scaler.transform(X_valid_spec)\n",
    "        X_test_spec = scaler.transform(X_test_spec)\n",
    "        X_val_spec = scaler.transform(X_val_spec)\n",
    "        # fit the classifier on training data\n",
    "        #svclassifier.fit(X_train, y_train)\n",
    "        \n",
    "        # compute performance measure (using test data)\n",
    "       # y_pred = svclassifier.predict(X_test)\n",
    "        \n",
    "    # training the model\n",
    "        classifier.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.01), metrics=['accuracy'])\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/classifier/classifier_with_al/certainty_sampling/classifier_domain_6.h5\")\n",
    "        #print(X_train_gen1.shape, X_train_spec.shape,y_train1.shape)\n",
    "        history = classifier.fit([np.expand_dims(np.asarray(X_train_gen1).astype(np.float32), 1), np.expand_dims(np.asarray(X_train_spec).astype(np.float32), 1)], np.asarray(y_train1).astype(np.float32), epochs=20,validation_data = ([np.expand_dims(np.asarray(X_val_gen).astype(np.float32), 1), np.expand_dims(np.asarray(X_val_spec).astype(np.float32), 1)], np.asarray(y_val_spec).astype(np.float32)), callbacks = [checkpoint, es], batch_size=64)\n",
    "\n",
    "# evaluating the model\n",
    "       # y_pred = classifier.predict([np.expand_dims(X_test_gen, 1), np.expand_dims(X_test_spec, 1)], verbose=0) \n",
    "        y_pred = classifier.predict([np.expand_dims(np.asarray(X_test_gen).astype(np.float32), 1), np.expand_dims(np.asarray(X_test_spec).astype(np.float32), 1)], verbose=0)\n",
    "        #print(y_pred)\n",
    "        y_pred = np.where(y_pred >= 0.5, 1,0)\n",
    "        \n",
    "        #print(y_pred)\n",
    "        y_pred = np.squeeze(y_pred)\n",
    "        #print(X_valid_gen1.shape)\n",
    "        get_prob = classifier.predict([np.expand_dims(np.asarray(X_valid_gen1).astype(np.float32), 1), np.expand_dims(np.asarray(X_valid_spec).astype(np.float32), 1)], verbose=0) \n",
    "       # print(get_prob)\n",
    "        \n",
    "        get_prob = np.squeeze(get_prob)\n",
    "       # print(get_prob)\n",
    "        #print( 'Final accuracy score: '+str(y_pred_valid[1]))\n",
    "        #print(y_test_spec, y_pred)\n",
    "       # f1 = f1_score(y_test_spec, y_pred, average='macro')\n",
    "        y_pred_valid = classifier.predict([np.expand_dims(np.asarray(X_valid_gen1).astype(np.float32), 1), np.expand_dims(np.asarray(X_valid_spec).astype(np.float32), 1)], verbose=0) \n",
    "        #print(y_pred_valid)\n",
    "        y_pred_valid = np.where(y_pred >= 0.5, 1,0)\n",
    "        y_pred_valid = np.squeeze(y_pred_valid)\n",
    "        \n",
    "        #print(y_test, y_pred)\n",
    "        # compute uncertainty metric (using validation data)\n",
    "        #classifier_export = classifier.export_model()\n",
    "        #model_export.predict -> return probability\n",
    "        #get_prob = classifier_export_predict([np.expand_dims(X_test_gen, 1), np.expand_dims(X_test_spec, 1)], y_test_spec, verbose=0) \n",
    "        #y_pred_valid = classifier.predict(X_valid)\n",
    "        #prob0 = []\n",
    "        #prob1=[]\n",
    "        prob = []\n",
    "        #print(prob)\n",
    "        #print(get_prob.shape)\n",
    "        #print(y_pred_valid.shape)\n",
    "        #print(get_prob.shape)\n",
    "        for i in range(get_prob.shape[0]):\n",
    "            #prob.append((-get_prob[i] * np.log2(get_prob[i])).sum(axis=1))\n",
    "            #print(get_prob[i])\n",
    "            if get_prob[i]<0.5:\n",
    "                prob.append(1-get_prob[i])\n",
    "            else:    \n",
    "                prob.append(get_prob[i])\n",
    "   \n",
    "            #print(prob)\n",
    "        \n",
    "        if uncertainty_sampling == True:\n",
    "           # prob = filter(lambda x: x <= 0.7, prob)\n",
    "            #prob2 = filter(lambda x: x <= 1, prob)\n",
    "            #print(prob2)\n",
    "            #prob = [x for x in prob if x < 0.8]\n",
    "            \n",
    "            ind = np.argsort(prob)[:k]\n",
    "            #ind1 = np.argsort(prob1)[:100]\n",
    "            #ind = ind0 + ind1\n",
    "           # print(ind0, ind1)\n",
    "            #print(ind)\n",
    "            #for i in ind:\n",
    "             #   print(prob[i])\n",
    "            #ind = np.argsort(prob)[:k]\n",
    "        else:\n",
    "            ind = np.argsort(prob)[-k:]\n",
    "        #print(y_train.shape,y_valid_gen[ind].shape)\n",
    "        # move samples from the validation set to the training set\n",
    "        X_train_gen = np.vstack((X_train_gen1,X_valid_gen1[ind]))\n",
    "        y_train_gen = np.hstack((y_train1, y_valid_gen1[ind])) \n",
    "       # print(X_valid_gen1.shape)\n",
    "        X_valid_gen = np.delete(X_valid_gen1, ind, axis = 0)      \n",
    "        y_valid_gen = np.delete(y_valid_gen1, ind, axis = 0)  \n",
    "        #print(X_valid_gen.shape)\n",
    "        X_train_spec = np.vstack((X_train_spec,X_valid_spec[ind]))\n",
    "        y_train_spec = np.hstack((y_train_spec, y_valid_spec[ind]))     \n",
    "        X_valid_spec = np.delete(X_valid_spec, ind, axis = 0)      \n",
    "        y_valid_spec = np.delete(y_valid_spec, ind, axis = 0) \n",
    "        \n",
    "        # inverse scaling\n",
    "        X_train_gen = scaler.inverse_transform(X_train_gen) \n",
    "        X_valid_gen = scaler.inverse_transform(X_valid_gen)\n",
    "        X_test_gen = scaler.inverse_transform(X_test_gen)\n",
    "        X_val_gen =scaler.inverse_transform(X_val_gen)\n",
    "          # inverse scaling\n",
    "        X_train_spec = scaler.inverse_transform(X_train_spec) \n",
    "        X_valid_spec = scaler.inverse_transform(X_valid_spec)\n",
    "        X_test_spec = scaler.inverse_transform(X_test_spec)\n",
    "        X_val_spec =scaler.inverse_transform(X_val_spec)\n",
    "        \n",
    "        # check stopping criteria\n",
    "        print( X_train_spec.shape[0])\n",
    "        #print(y_train_gen, y_train_spec)\n",
    "        if X_train_spec.shape[0] > max_query1:\n",
    "            continue_al = False\n",
    "            \n",
    "   # print(X_test_gen.shape, y_test_gen.shape, X_test_spec.shape, y_test_spec.shape)    \n",
    "   # print(X_val_gen.shape, y_val_gen.shape, X_val_spec.shape, y_val_spec.shape) \n",
    "    return X_train_gen,X_train_spec, y_train_spec, X_val_gen, X_val_spec, y_val_spec, X_test_gen, X_test_spec, y_test_spec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data for the general sentence embeddings, here corresponding data from domain 0 was chosen\n",
    "def return_results_AL(k, par0, par1): \n",
    "    # set the target domain\n",
    "    index_spec = k\n",
    "    print(k)\n",
    "    with open('data/sentence_embeddings/general/sorted/train/train_data6_'+str(k)+'.p', 'rb') as f:\n",
    "        X_train_gen_all = pkl.load(f)\n",
    "    with open('data/sentence_embeddings/general/sorted/train/train_labels6_'+str(k)+'.p', 'rb') as f:\n",
    "        y_train_gen_all = pkl.load(f)\n",
    "    with open('data/sentence_embeddings/general/sorted/val_test/vt_data6_'+str(k)+'.p', 'rb') as f:\n",
    "        X_val_test_spec = pkl.load(f)\n",
    "    with open('data/sentence_embeddings/general/sorted/val_test/vt_labels6_'+str(k)+'.p', 'rb') as f:\n",
    "        y_val_test = pkl.load(f)   \n",
    "    labels_total = np1.hstack((y_train_gen_all[:,:4200].astype(int), y_val_test))\n",
    "    X_val_gen, X_test_gen = X_val_test_spec[:600], X_val_test_spec[600:]\n",
    "    y_train_gen_all = y_train_gen_all[0,:]\n",
    "    y_train, y_val, y_test = y_train_gen_all[:4200], y_val_test[0,:600], y_val_test[0,600:]\n",
    "\n",
    "# import the data from the specific sentence embeddings, here corresponding data from domain 0 was chosen\n",
    "    with open('data/sentence_embeddings/specific/sentemb/sentemb_unlabeled6_'+str(k)+'.p', 'rb') as f:\n",
    "        X_spec = pkl.load(f)   \n",
    "    X_spec=np.repeat(X_spec,repeats=3, axis=1)\n",
    "    X_train_spec, X_val_spec, X_test_spec = X_spec.transpose()[:4200], X_spec.transpose()[4200:4800], X_spec.transpose()[4800:\n",
    "   \n",
    "    # load the original, unsorted data\n",
    "    with open('data/sentence_embeddings/general/unsorted/sentemb/sentemb_unlabeled6.p', 'rb') as f:\n",
    "        data_general = pkl.load(f)\n",
    "    with open('data/sentence_embeddings/general/unsorted/label_domain/label_domain_train_sentemb_unlabeled6.p', 'rb') as f:\n",
    "        labels_train = pkl.load(f)\n",
    "    with open('data/sentence_embeddings/general/unsorted/label_domain/label_domain_test_sentemb_unlabeled6.p', 'rb') as f:\n",
    "        labels_test = pkl.load(f)\n",
    "    labels_general = np.hstack((labels_train, labels_test))\n",
    "    data_general = data_general.transpose()\n",
    "\n",
    "# load the cleaned data\n",
    "    with open('data/cleaned_data/merged_cleaned.p', 'rb') as f:\n",
    "        df_train = pkl.load(f)\n",
    "    with open('data/cleaned_data/test_cleaned.p', 'rb') as f:\n",
    "        df_test = pkl.load(f)\n",
    "    list_unlabel = df_train.index[df_train['label'] == 3].to_list()\n",
    "    df_train = df_train[~df_train.index.isin(list_unlabel)].reset_index(drop=True)\n",
    "\n",
    "# create a list of data frames dfs, each data frame represents one domain\n",
    "    df = pd.concat([df_train, df_test],ignore_index=True)\n",
    "    dfs = [x for _, x in df.groupby('domain')]\n",
    "\n",
    "# get the word distribution of each domain\n",
    "# the frequency of each existing word is computed in every domain\n",
    "    import collections\n",
    "    import regex as re\n",
    "    word_counter = []\n",
    "    for df in dfs:\n",
    "        counts = collections.Counter()\n",
    "        words = re.compile(r'\\w+')\n",
    "        reviews = np.array([s for s in df['text']])\n",
    "        for review in reviews:\n",
    "            counts.update(words.findall(review.lower()))\n",
    "        word_counter.append(counts)\n",
    "\n",
    "# the rows of df are the 16 domains, the columns are all existing words\n",
    "# the number of the cells of df is the word frequency for the word in the domain\n",
    "    df_dist = pd.DataFrame(word_counter)\n",
    "    df_dist = df_dist.fillna(0) \n",
    "\n",
    "# get list js_d of jensen_shannon distances to the target domain\n",
    "    js_d = []\n",
    "    for i in range(df_dist.shape[0]):\n",
    "        d = distance.jensenshannon(np.array(df_dist.iloc[index_spec]), np.array(df_dist.iloc[i]))\n",
    "        js_d.append(d)\n",
    "    \n",
    "# take 5 most similiar distributions\n",
    "# most_sim_dist is a list of 5 elements with the 5 closest domains to the target domain\n",
    "    most_sim_dist = sorted(range(len(js_d)), key=lambda i: js_d[i], reverse=True)[-5:]\n",
    "    most_sim_dist.remove(index_spec)\n",
    "\n",
    "# remove general embeddings that aren't from these 5 domains\n",
    "    index_to_keep = [index for index, value in enumerate(labels_general[1]) if int(value) in most_sim_dist]\n",
    "    labels_general, data_general = labels_general[:, index_to_keep], data_general[index_to_keep]\n",
    "    labels_general = labels_general[0,:]\n",
    "    labels_general = labels_general.transpose()\n",
    "# data splitting\n",
    "    X_train_gen, X_val_gen, X_test_gen = data_general[:5000], data_general[5000:6000], data_general[6000:]\n",
    "    y_train_gen, y_val_gen, y_test_gen = labels_general[:5000], labels_general[5000:6000], labels_general[6000:]\n",
    "                                                                                                                         \n",
    "    #THE LSTM Classifier\n",
    "    INPUT_SIZE = 300\n",
    "    LATENT_SIZE = 300\n",
    "\n",
    "# domain-general model parts\n",
    "    inp_gen = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "# domain-specific model parts\n",
    "    inp_spec = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "# concatenate domain-general and domain-specific results\n",
    "    merged = tf.keras.layers.Concatenate()([inp_gen, inp_spec])\n",
    "    merged = tf.keras.layers.Dense(100, activation='sigmoid')(merged)\n",
    "    merged = tf.keras.layers.Dropout(.5)(merged)\n",
    "    merged = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "    classifier = tf.keras.Model([inp_gen,inp_spec], merged)\n",
    "\n",
    "    # apply AL on specific and general sentence embeddings\n",
    "# make sure to pick the right parameters for uncertainty_sampling and outlier_detection \n",
    "# SIDENOTE: for certainty sampling experiment change max_query to 2200 (for domain 4 even to 3000) for general embeddings - due to different number of positive/negative samples \n",
    "    classifier.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.01), metrics=['accuracy'])\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/classifier/classifier_with_al/certainty_sampling/classifier_domain_6.h5\")\n",
    "    history = classifier.fit([np.expand_dims(np.asarray(X_train_gen_al).astype(np.float32), 1), np.expand_dims(np.asarray(X_train_spec_al).astype(np.float32), 1)], np.asarray(y_train_spec_al).astype(np.float32), epochs=20, validation_data = ([np.expand_dims(np.asarray(X_val_gen2).astype(np.float32), 1), np.expand_dims(np.asarray(X_val_spec).astype(np.float32), 1)], np.asarray(y_val).astype(np.float32)), callbacks = [checkpoint, es], batch_size=64)\n",
    "\n",
    "# evaluating the model\n",
    "    score = classifier.evaluate([np.expand_dims(np.asarray(X_test_gen2).astype(np.float32), 1), np.expand_dims(np.asarray(X_test_spec).astype(np.float32), 1)], np.asarray(y_test).astype(np.float32), verbose=0) \n",
    "    return(k, pars[0], pars[1], 'Final accuracy score: '+str(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peirama 1 0 (1400, 2000)\n",
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pkl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3625328/1234503671.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpars\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_results_AL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m            \u001b[0;31m# data = pd.DataFrame(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3625328/2499062473.py\u001b[0m in \u001b[0;36mreturn_results_AL\u001b[0;34m(k, par0, par1)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/sentence_embeddings/general/sorted/train/train_data6_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mX_train_gen_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/sentence_embeddings/general/sorted/train/train_labels6_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pkl' is not defined"
     ]
    }
   ],
   "source": [
    "class TestCase:\n",
    "    def __init__(self, name, i_range, pars):\n",
    "        self.name = name\n",
    "        self.i_range = i_range\n",
    "        self.pars = pars\n",
    "        \n",
    "test_cases = [\n",
    "    TestCase(\"peirama 1\", range(0,16), [(1400, 2000)]),\n",
    "#     TestCase(\"peirama 2\", range(0,16), [(1400, 2200), (2100, 3000)]),\n",
    "   # TestCase(\"peirama 3\", range(0,100), [(1400, 2200), (2100, 3000)]),\n",
    "]\n",
    "\n",
    "for test_case in test_cases:\n",
    "    for i in test_case.i_range:\n",
    "        for pars in test_case.pars:\n",
    "            print(test_case.name, i, pars)\n",
    "            x = return_results_AL(i, pars[0], pars[1])\n",
    "            print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the results\n",
    "import matplotlib.pyplot as plt\n",
    "#from tf.keras.utils import plot_model\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, to_file='TRIAL_2_AL_full.png')\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'ro', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing the data for the general sentence embeddings, here corresponding data from domain 0 was chosen\n",
    "def return_results_AL(k, par0, par1): \n",
    "    # set the target domain\n",
    "    index_spec = k\n",
    "    print(k)\n",
    "    with open('data/sentence_embeddings/general/sorted/train/train_data6_'+str(k)+'.p', 'rb') as f:\n",
    "        X_train_gen_all = pkl.load(f)\n",
    "\n",
    "    with open('data/sentence_embeddings/general/sorted/train/train_labels6_'+str(k)+'.p', 'rb') as f:\n",
    "        y_train_gen_all = pkl.load(f)\n",
    "\n",
    "    with open('data/sentence_embeddings/general/sorted/val_test/vt_data6_'+str(k)+'.p', 'rb') as f:\n",
    "        X_val_test_spec = pkl.load(f)\n",
    "\n",
    "    with open('data/sentence_embeddings/general/sorted/val_test/vt_labels6_'+str(k)+'.p', 'rb') as f:\n",
    "        y_val_test = pkl.load(f)\n",
    "    #print(y_train_gen_all[:,:4200])\n",
    "    #labels_total = np1.hstack((y_train_gen_all[:,:4200], y_val_test))\n",
    "    \n",
    "    labels_total = np1.hstack((y_train_gen_all[:,:4200].astype(int), y_val_test))\n",
    "    X_val_gen, X_test_gen = X_val_test_spec[:600], X_val_test_spec[600:]\n",
    "    y_train_gen_all = y_train_gen_all[0,:]\n",
    "    y_train, y_val, y_test = y_train_gen_all[:4200], y_val_test[0,:600], y_val_test[0,600:]\n",
    "\n",
    "\n",
    "# import the data from the specific sentence embeddings, here corresponding data from domain 0 was chosen\n",
    "    with open('data/sentence_embeddings/specific/sentemb/sentemb_unlabeled6_'+str(k)+'.p', 'rb') as f:\n",
    "        X_spec = pkl.load(f)\n",
    "    \n",
    "#X_train_spec, X_val_spec, X_test_spec = X_spec[:1400], X_spec[1400:1600], X_spec[1600:2000] \n",
    "\n",
    "    import numpy as np\n",
    "    X_spec=np.repeat(X_spec,repeats=3, axis=1)\n",
    "\n",
    "\n",
    "    X_train_spec, X_val_spec, X_test_spec = X_spec.transpose()[:4200], X_spec.transpose()[4200:4800], X_spec.transpose()[4800:]\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "    # load the original, unsorted data\n",
    "    with open('data/sentence_embeddings/general/unsorted/sentemb/sentemb_unlabeled6.p', 'rb') as f:\n",
    "        data_general = pkl.load(f)\n",
    "\n",
    "    with open('data/sentence_embeddings/general/unsorted/label_domain/label_domain_train_sentemb_unlabeled6.p', 'rb') as f:\n",
    "        labels_train = pkl.load(f)\n",
    "    \n",
    "    with open('data/sentence_embeddings/general/unsorted/label_domain/label_domain_test_sentemb_unlabeled6.p', 'rb') as f:\n",
    "        labels_test = pkl.load(f)\n",
    "    \n",
    "    labels_general = np.hstack((labels_train, labels_test))\n",
    "\n",
    "    data_general = data_general.transpose()\n",
    "\n",
    "# load the cleaned data\n",
    "    with open('data/cleaned_data/merged_cleaned.p', 'rb') as f:\n",
    "        df_train = pkl.load(f)\n",
    "    with open('data/cleaned_data/test_cleaned.p', 'rb') as f:\n",
    "        df_test = pkl.load(f)\n",
    "\n",
    "# create a list of data frames dfs, each data frame represents one domain\n",
    "#df = pd.concat([df_train, df_test],ignore_index=True)\n",
    "#dfs = [x for _, x in df.groupby('domain')]\n",
    "\n",
    "    list_unlabel = df_train.index[df_train['label'] == 3].to_list()\n",
    "\n",
    "    df_train = df_train[~df_train.index.isin(list_unlabel)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# create a list of data frames dfs, each data frame represents one domain\n",
    "    df = pd.concat([df_train, df_test],ignore_index=True)\n",
    "    dfs = [x for _, x in df.groupby('domain')]\n",
    "\n",
    "# get the word distribution of each domain\n",
    "# the frequency of each existing word is computed in every domain\n",
    "    import collections\n",
    "    import regex as re\n",
    "    word_counter = []\n",
    "    for df in dfs:\n",
    "        counts = collections.Counter()\n",
    "        words = re.compile(r'\\w+')\n",
    "        reviews = np.array([s for s in df['text']])\n",
    "        for review in reviews:\n",
    "            counts.update(words.findall(review.lower()))\n",
    "        word_counter.append(counts)\n",
    "\n",
    "# the rows of df are the 16 domains, the columns are all existing words\n",
    "# the number of the cells of df is the word frequency for the word in the domain\n",
    "    df_dist = pd.DataFrame(word_counter)\n",
    "    df_dist = df_dist.fillna(0)\n",
    " \n",
    "\n",
    "\n",
    "# get list js_d of jensen_shannon distances to the target domain\n",
    "    js_d = []\n",
    "    for i in range(df_dist.shape[0]):\n",
    "        d = distance.jensenshannon(np.array(df_dist.iloc[index_spec]), np.array(df_dist.iloc[i]))\n",
    "        js_d.append(d)\n",
    "    \n",
    "# take 5 most similiar distributions\n",
    "# most_sim_dist is a list of 5 elements with the 5 closest domains to the target domain\n",
    "    most_sim_dist = sorted(range(len(js_d)), key=lambda i: js_d[i], reverse=True)[-5:]\n",
    "    most_sim_dist.remove(index_spec)\n",
    "\n",
    "# remove general embeddings that aren't from these 5 domains\n",
    "    index_to_keep = [index for index, value in enumerate(labels_general[1]) if int(value) in most_sim_dist]\n",
    "    labels_general, data_general = labels_general[:, index_to_keep], data_general[index_to_keep]\n",
    "\n",
    "\n",
    "# get indices for sorting the array\n",
    "   # ind = sort_array(labels_general, labels_total)\n",
    "\n",
    "# sort general sentence embeddings\n",
    "    #data_general, labels_general = data_general[ind], labels_general[:, ind]\n",
    "    #print(labels_general, labels_total)\n",
    "    #labels_general.dtype\n",
    "    #print(labels_total.shape)\n",
    "    #print(labels_general.shape)\n",
    "    #print(labels_general.shape)\n",
    "   # ind = sort_array2(labels_general, labels_total)\n",
    "    #print(labels_general.shape)\n",
    "    #print(ind.shape)\n",
    "# sort general sentence embeddings\n",
    "    #print(labels_general.shape)\n",
    "    #data_general, labels_general = data_general[ind], labels_general[:, ind]\n",
    "    print(labels_general)\n",
    "    labels_general = labels_general[0,:]\n",
    "    labels_general = labels_general.transpose()\n",
    "    #print(labels_general)\n",
    "# data splitting\n",
    "    X_train_gen, X_val_gen, X_test_gen = data_general[:5000], data_general[5000:6000], data_general[6000:7700]\n",
    "   # print(data_general, labels_general)\n",
    "    y_train_gen, y_val_gen, y_test_gen = labels_general[:5000], labels_general[5000:6000], labels_general[6000:7700]\n",
    "    \n",
    "# data splitting\n",
    "    #print( X_train_gen.shape, y_train_gen.shape, np.vstack((X_val_gen,X_test_gen)).shape, np.hstack((y_val_gen,y_test_gen)).shape)\n",
    "    #X_train_spec_al, y_train_spec_al = AL(X_train_spec, y_train, np.vstack((X_val_spec,X_test_spec)), np.hstack((y_val,y_test)), pars[0], True, 2)\n",
    "    #X_train_gen_al, y_train_gen_al = AL( X_train_gen, y_train_gen , np.vstack((X_val_gen1,X_test_gen1)), np.hstack((y_val_gen1,y_test_gen1)), pars[1], True, 2)\n",
    "    X_train_gen_al,X_train_spec_al, y_train_gen_al, y_train_spec_al  = AL( X_train_gen, X_train_spec, y_train_gen, y_train, np.vstack((X_val_gen,X_test_gen)), np.vstack((X_val_spec,X_test_spec)), np.hstack((y_val_gen,y_test_gen)), np.hstack((y_val_gen,y_test_gen)),np.hstack((y_val,y_test)),pars[0], pars[1], True, 2)\n",
    "    #print(X_train_gen_al.shape,X_train_spec_al.shape, y_train_gen_al.shape, y_train_spec_al)\n",
    "    #print( X_train_gen_al.shape, y_train_spec_al.shape)\n",
    "    #print(y_train_gen_al.shape, y_train_spec_al.shape)\n",
    "    ind = sort_array(y_train_gen_al, y_train_spec_al)\n",
    "    #print( X_train_gen_al.shape, y_train_spec_al.shape)\n",
    "    X_train_gen_al, y_train = X_train_gen_al[ind], y_train_spec_al\n",
    "    #y_train=y_train_spec_al \n",
    "    ind2 = sort_array(y_val_gen, y_val)\n",
    "    #print( X_train_gen_al.shape, y_train_spec_al.shape)\n",
    "    X_val_gen = X_val_gen[ind2]\n",
    "    ind3 = sort_array(y_test_gen, y_test)\n",
    "    X_test_gen = X_test_gen[ind3]\n",
    "\n",
    "\n",
    "    #THE LSTM Classifier\n",
    "\n",
    "    INPUT_SIZE = 300\n",
    "    LATENT_SIZE = 300\n",
    "\n",
    "# domain-general model parts\n",
    "    inp_gen = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "    #out_gen = tf.keras.layers.Dense(300, activation='sigmoid')(inp_gen)\n",
    "#\n",
    "    #out_gen = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_gen)\n",
    "    #out_gen, attn_weights_gen = SeqSelfAttention(return_attention = True)(out_gen1)\n",
    "# domain-specific model parts\n",
    "    inp_spec = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "    #out_spec = tf.keras.layers.Dense(300, activation='sigmoid')(inp_spec)\n",
    "#inp_spec_att, attn_weights_spec = SeqSelfAttention(return_attention = True)(inp_spec)\n",
    "   # out_spec = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_spec)\n",
    "   # out_spec, attn_weights_spec = SeqSelfAttention(return_attention = True)(out_spec1)\n",
    "# concatenate domain-general and domain-specific results\n",
    "    merged = tf.keras.layers.Concatenate()([inp_gen, inp_spec])\n",
    "    merged = tf.keras.layers.Dense(300, activation='sigmoid')(merged)\n",
    "\n",
    "\n",
    "    #merged = tf.keras.layers.Dense(100, activation='sigmoid')(merged)\n",
    "    #merged = tf.keras.layers.Dense(100, activation='sigmoid')(merged)\n",
    "# drop out layer and dense layer\n",
    "    merged = tf.keras.layers.Dropout(.2)(merged)\n",
    "    merged = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    classifier = tf.keras.Model([inp_gen,inp_spec], merged)\n",
    "    #classifier.summary()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # apply AL on specific and general sentence embeddings\n",
    "# make sure to pick the right parameters for uncertainty_sampling and outlier_detection \n",
    "# SIDENOTE: for certainty sampling experiment change max_query to 2200 (for domain 4 even to 3000) for general embeddings - due to different number of positive/negative samples\n",
    "  \n",
    "    \n",
    "    #print(X_train_gen_al.shape, X_train_spec_al.shape, y_train.shape, X_val_gen.shape, X_val_spec.shape, y_val.shape)\n",
    "    # training the model\n",
    "    classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/classifier/classifier_with_al/certainty_sampling/classifier_domain_3.h5\")\n",
    "    history = classifier.fit([np.expand_dims(X_train_gen_al, 1), np.expand_dims(X_train_spec_al, 1)], y_train, epochs=50, validation_data = ([np.expand_dims(X_val_gen, 1), np.expand_dims(X_val_spec, 1)], y_val), callbacks = [checkpoint, es], batch_size=32)\n",
    "\n",
    "# evaluating the model\n",
    "    print(X_test_gen.shape, X_test_spec.shape, y_test.shape)\n",
    "    score = classifier.evaluate([np.expand_dims(X_test_gen, 1), np.expand_dims(X_test_spec, 1)], y_test, verbose=0) \n",
    "    print(k)\n",
    "    return(k, pars[0], pars[1], 'Final accuracy score: '+str(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np1\n",
    "# importing the data for the general sentence embeddings, here corresponding data from domain 0 was chosen\n",
    "def return_results_AL(k): \n",
    "    # set the target domain\n",
    "    index_spec = k\n",
    "    print(k)\n",
    "    with open('data/sentence_embeddings/general/sorted/train/train_data8_'+str(k)+'.p', 'rb') as f:\n",
    "        X_train_gen_all = pkl.load(f)\n",
    "\n",
    "    with open('data/sentence_embeddings/general/sorted/train/train_labels8_'+str(k)+'.p', 'rb') as f:\n",
    "        y_train_gen_all = pkl.load(f)\n",
    "\n",
    "    with open('data/sentence_embeddings/general/sorted/val_test/vt_data8_'+str(k)+'.p', 'rb') as f:\n",
    "        X_val_test_spec = pkl.load(f)\n",
    "\n",
    "    with open('data/sentence_embeddings/general/sorted/val_test/vt_labels8_'+str(k)+'.p', 'rb') as f:\n",
    "        y_val_test = pkl.load(f)\n",
    "\n",
    "    labels_total = np1.hstack((y_train[:,:4200], y_val_test))\n",
    "    #labels_total = np.hstack((y_train_gen_all[:,:4200], y_val_test))\n",
    "    X_val_gen, X_test_gen = X_val_test_spec[:600], X_val_test_spec[600:]\n",
    "    y_train_gen_all = y_train_gen_all[0,:]\n",
    "    y_train, y_val, y_test = y_train_gen_all[:4200], y_val_test[0,:600], y_val_test[0,600:]\n",
    "\n",
    "# import the data from the specific sentence embeddings, here corresponding data from domain 0 was chosen\n",
    "    with open('data/sentence_embeddings/specific/sentemb/sentemb_unlabeled8_'+str(k)+'.p', 'rb') as f:\n",
    "        X_spec = pkl.load(f)\n",
    "    \n",
    "#X_train_spec, X_val_spec, X_test_spec = X_spec[:1400], X_spec[1400:1600], X_spec[1600:2000] \n",
    "\n",
    "\n",
    "    X_spec=np.repeat(X_spec,repeats=3, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    X_train_spec, X_val_spec, X_test_spec = X_spec.transpose()[:4200], X_spec.transpose()[4200:4800], X_spec.transpose()[4800:]\n",
    "    \n",
    "    \n",
    "\n",
    "    # load the original, unsorted data\n",
    "    with open('data/sentence_embeddings/general/unsorted/sentemb/sentemb_unlabeled8.p', 'rb') as f:\n",
    "        data_general = pkl.load(f)\n",
    "\n",
    "    with open('data/sentence_embeddings/general/unsorted/label_domain/label_domain_train_sentemb_unlabeled8.p', 'rb') as f:\n",
    "        labels_train = pkl.load(f)\n",
    "    \n",
    "    with open('data/sentence_embeddings/general/unsorted/label_domain/label_domain_test_sentemb_unlabeled8.p', 'rb') as f:\n",
    "        labels_test = pkl.load(f)\n",
    "    \n",
    "    labels_general = np1.hstack((labels_train, labels_test))\n",
    "\n",
    "    data_general = data_general.transpose()\n",
    "\n",
    "# load the cleaned data\n",
    "    with open('data/cleaned_data/merged_cleaned.p', 'rb') as f:\n",
    "        df_train = pkl.load(f)\n",
    "    with open('data/cleaned_data/test_cleaned.p', 'rb') as f:\n",
    "        df_test = pkl.load(f)\n",
    "\n",
    "# create a list of data frames dfs, each data frame represents one domain\n",
    "#df = pd.concat([df_train, df_test],ignore_index=True)\n",
    "#dfs = [x for _, x in df.groupby('domain')]\n",
    "\n",
    "    list_unlabel = df_train.index[df_train['label'] == 3].to_list()\n",
    "\n",
    "    df_train = df_train[~df_train.index.isin(list_unlabel)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# create a list of data frames dfs, each data frame represents one domain\n",
    "    df = pd.concat([df_train, df_test],ignore_index=True)\n",
    "    dfs = [x for _, x in df.groupby('domain')]\n",
    "\n",
    "# get the word distribution of each domain\n",
    "# the frequency of each existing word is computed in every domain\n",
    "    import collections\n",
    "    import regex as re\n",
    "    word_counter = []\n",
    "    for df in dfs:\n",
    "        counts = collections.Counter()\n",
    "        words = re.compile(r'\\w+')\n",
    "        reviews = np.array([s for s in df['text']])\n",
    "        for review in reviews:\n",
    "            counts.update(words.findall(review.lower()))\n",
    "        word_counter.append(counts)\n",
    "\n",
    "# the rows of df are the 16 domains, the columns are all existing words\n",
    "# the number of the cells of df is the word frequency for the word in the domain\n",
    "    df_dist = pd.DataFrame(word_counter)\n",
    "    df_dist = df_dist.fillna(0)\n",
    " \n",
    "\n",
    "\n",
    "# get list js_d of jensen_shannon distances to the target domain\n",
    "    js_d = []\n",
    "    for i in range(df_dist.shape[0]):\n",
    "        d = distance.jensenshannon(np.array(df_dist.iloc[index_spec]), np.array(df_dist.iloc[i]))\n",
    "        js_d.append(d)\n",
    "    \n",
    "# take 5 most similiar distributions\n",
    "# most_sim_dist is a list of 5 elements with the 5 closest domains to the target domain\n",
    "    most_sim_dist = sorted(range(len(js_d)), key=lambda i: js_d[i], reverse=True)[-5:]\n",
    "    most_sim_dist.remove(index_spec)\n",
    "\n",
    "# remove general embeddings that aren't from these 5 domains\n",
    "    index_to_keep = [index for index, value in enumerate(labels_general[1]) if int(value) in most_sim_dist]\n",
    "    labels_general, data_general = labels_general[:, index_to_keep], data_general[index_to_keep]\n",
    "    # function for sorting two arrays such that both arrays have the same labels\n",
    "# returns indeces_sorted which consists of indices and is used for sorting array_to_sort\n",
    "  \n",
    "    \n",
    "      \n",
    "\n",
    "    print(labels_train.shape, labels_total_train_val.shape)\n",
    "    ind_train = sort_array(labels_train, labels_total_train_val)\n",
    "    #print(ind.shape)\n",
    "# sort general sentence embeddings\n",
    "    data_general_train, labels_train = data_general_train[ind_train], labels_train[:, ind_train]\n",
    "\n",
    "    #print(data_general_train.shape)\n",
    "    \n",
    "    ind_test = sort_array(labels_test, labels_total_test)\n",
    "    #print(ind.shape)\n",
    "# sort general sentence embeddings\n",
    "    data_general_test, labels_test = data_general_test[ind_test], labels_test[:, ind_test]\n",
    "# data splitting\n",
    "    #print(X_train_spec.shape)\n",
    "    X_train_gen, X_val_gen = data_general_train[:4200], data_general_train[4200:]\n",
    "\n",
    "# data splitting\n",
    "    X_train_gen, X_val_gen, X_test_gen = data_general[:4200], data_general[4200:4800], data_general[4800:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #THE LSTM Classifier\n",
    "\n",
    "    INPUT_SIZE = 300\n",
    "    LATENT_SIZE = 300\n",
    "\n",
    "# domain-general model parts\n",
    "    inp_gen = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "    #out_gen = tf.keras.layers.Dense(300, activation='sigmoid')(inp_gen)\n",
    "#\n",
    "    #out_gen = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_gen)\n",
    "    #out_gen, attn_weights_gen = SeqSelfAttention(return_attention = True)(out_gen1)\n",
    "# domain-specific model parts\n",
    "    inp_spec = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "    #out_spec = tf.keras.layers.Dense(300, activation='sigmoid')(inp_spec)\n",
    "#inp_spec_att, attn_weights_spec = SeqSelfAttention(return_attention = True)(inp_spec)\n",
    "   # out_spec = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_spec)\n",
    "   # out_spec, attn_weights_spec = SeqSelfAttention(return_attention = True)(out_spec1)\n",
    "# concatenate domain-general and domain-specific results\n",
    "    merged = tf.keras.layers.Concatenate()([inp_gen, inp_spec])\n",
    "    merged = tf.keras.layers.Dense(300, activation='sigmoid')(merged)\n",
    "\n",
    "\n",
    "    #merged = tf.keras.layers.Dense(100, activation='sigmoid')(merged)\n",
    "    #merged = tf.keras.layers.Dense(100, activation='sigmoid')(merged)\n",
    "# drop out layer and dense layer\n",
    "    merged = tf.keras.layers.Dropout(.4)(merged)\n",
    "    merged = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    classifier = tf.keras.Model([inp_gen,inp_spec], merged)\n",
    "    #classifier.summary()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # apply AL on specific and general sentence embeddings\n",
    "# make sure to pick the right parameters for uncertainty_sampling and outlier_detection \n",
    "# SIDENOTE: for certainty sampling experiment change max_query to 2200 (for domain 4 even to 3000) for general embeddings - due to different number of positive/negative samples\n",
    "  \n",
    "    \n",
    "    \n",
    "  # training the model\n",
    "    classifier.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/classifier/classifier_with_al/random/classifier_domain3_15.h5\")\n",
    "    history = classifier.fit([np.expand_dims(X_train_gen[:1400], 1), np.expand_dims(X_train_spec[:1400], 1)], y_train[:1400], epochs=30, validation_data = ([np.expand_dims(X_val_gen, 1), np.expand_dims(X_val_spec, 1)], y_val), callbacks = [checkpoint, es], batch_size=32)\n",
    "\n",
    "# evaluating the model\n",
    "    score = classifier.evaluate([np.expand_dims(X_test_gen, 1), np.expand_dims(X_test_spec, 1)], y_test, verbose=0) \n",
    "    print('Final accuracy score: '+str(score[1]))\n",
    "    return(k, 'Final accuracy score: '+str(score[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peirama 1 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "return_results_AL() missing 2 required positional arguments: 'par0' and 'par1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-806-8f64e5bc5929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;31m# for pars in test_case.pars:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_results_AL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: return_results_AL() missing 2 required positional arguments: 'par0' and 'par1'"
     ]
    }
   ],
   "source": [
    " class TestCase:\n",
    "    def __init__(self, name, i_range):\n",
    "        self.name = name\n",
    "        self.i_range = i_range\n",
    "       # self.pars = pars\n",
    "        \n",
    "test_cases = [\n",
    "    TestCase(\"peirama 1\", range(0,16)),\n",
    "#     TestCase(\"peirama 1\", range(0,16), [(1400, 2200), (2100, 3000)]),\n",
    "   # TestCase(\"peirama 2\", range(0,16), [1400, 2200])\n",
    "   # TestCase(\"peirama 3\", range(0,100), [(1400, 2200), (2100, 3000)]),\n",
    "]\n",
    "\n",
    "for test_case in test_cases:\n",
    "    for i in test_case.i_range:\n",
    "       # for pars in test_case.pars:\n",
    "        print(test_case.name, i)\n",
    "        x = return_results_AL(i)\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# importing the data for the general sentence embeddings, here corresponding data from domain 0 was chosen\n",
    "def return_results_AL(i, par0, par1): \n",
    "    with open('data/sentence_embeddings/general/sorted/train/train_data3_'+str(i)+'.p', 'rb') as f:\n",
    "        X_train_gen_all = pkl.load(f)\n",
    "\n",
    "    with open('data/sentence_embeddings/general/sorted/train/train_labels3_'+str(i)+'.p', 'rb') as f:\n",
    "        y_train_gen_all = pkl.load(f)\n",
    "\n",
    "    with open('data/sentence_embeddings/general/sorted/val_test/vt_data3_'+str(i)+'.p', 'rb') as f:\n",
    "        X_val_test_spec = pkl.load(f)\n",
    "\n",
    "    with open('data/sentence_embeddings/general/sorted/val_test/vt_labels3_'+str(i)+'.p', 'rb') as f:\n",
    "        y_val_test = pkl.load(f)\n",
    "\n",
    "    X_val_gen, X_test_gen = X_val_test_spec[:600], X_val_test_spec[600:]\n",
    "    y_train_gen_all = y_train_gen_all[0,:]\n",
    "    y_train, y_val, y_test = y_train_gen_all[:4200], y_val_test[0,:600], y_val_test[0,600:]\n",
    "\n",
    "\n",
    "# import the data from the specific sentence embeddings, here corresponding data from domain 0 was chosen\n",
    "    with open('data/sentence_embeddings/specific/sentemb/sentemb_unlabeled3_'+str(i)+'.p', 'rb') as f:\n",
    "        X_spec = pkl.load(f)\n",
    "    \n",
    "#X_train_spec, X_val_spec, X_test_spec = X_spec[:1400], X_spec[1400:1600], X_spec[1600:2000] \n",
    "\n",
    "    import numpy as np\n",
    "    X_spec=np.repeat(X_spec,repeats=3, axis=1)\n",
    "\n",
    "\n",
    "    X_train_spec, X_val_spec, X_test_spec = X_spec.transpose()[:4200], X_spec.transpose()[4200:4800], X_spec.transpose()[4800:]\n",
    "    \n",
    "    \n",
    " #THE LSTM Classifier\n",
    "\n",
    "\n",
    "    #THE LSTM Classifier\n",
    "\n",
    "    INPUT_SIZE = 300\n",
    "    LATENT_SIZE = 300\n",
    "\n",
    "# domain-general model parts\n",
    "    inp_gen = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "    #out_gen = tf.keras.layers.Dense(300, activation='sigmoid')(inp_gen)\n",
    "#\n",
    "    #out_gen = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_gen)\n",
    "    #out_gen, attn_weights_gen = SeqSelfAttention(return_attention = True)(out_gen1)\n",
    "# domain-specific model parts\n",
    "    inp_spec = tf.keras.Input(shape=(1,INPUT_SIZE))\n",
    "    #out_spec = tf.keras.layers.Dense(300, activation='sigmoid')(inp_spec)\n",
    "#inp_spec_att, attn_weights_spec = SeqSelfAttention(return_attention = True)(inp_spec)\n",
    "   # out_spec = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LATENT_SIZE, input_shape=(None,1,INPUT_SIZE)))(inp_spec)\n",
    "   # out_spec, attn_weights_spec = SeqSelfAttention(return_attention = True)(out_spec1)\n",
    "# concatenate domain-general and domain-specific results\n",
    "    merged = tf.keras.layers.Concatenate()([inp_gen, inp_spec])\n",
    "    merged = tf.keras.layers.Dense(300, activation='sigmoid')(merged)\n",
    "\n",
    "\n",
    "    #merged = tf.keras.layers.Dense(100, activation='sigmoid')(merged)\n",
    "    #merged = tf.keras.layers.Dense(100, activation='sigmoid')(merged)\n",
    "# drop out layer and dense layer\n",
    "    merged = tf.keras.layers.Dropout(.0)(merged)\n",
    "    merged = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    classifier = tf.keras.Model([inp_gen,inp_spec], merged)\n",
    "    #classifier.summary()\n",
    "\n",
    "    classifier = tf.keras.Model([inp_gen,inp_spec], merged)\n",
    "    #classifier.summary()\n",
    "    \n",
    "    #X_train_spec_al, y_train_spec_al = AL(X_train_spec, y_train, np.vstack((X_val_spec,X_test_spec)), np.hstack((y_val,y_test)), 1400, True, 2)\n",
    "    X_train_gen_al,  X_train_spec_al, y_train_gen_al, y_train_spec_al = AL(X_train_gen_all, X_train_spec, y_train_gen_all, y_train, np.vstack((X_val_gen,X_test_gen)), np.vstack((X_val_spec,X_test_spec)), np.hstack((y_val,y_test)), np.hstack((y_val,y_test)), pars[0], pars[1], True, 2)\n",
    "    # sort general sentence embeddings so that general and specific sentence embeddings have the same labels \n",
    "# and number of instances\n",
    "    ind = sort_array(y_train_gen_al, y_train_spec_al)\n",
    "    X_train_gen_al, y_train = X_train_gen_al[ind], y_train_spec_al\n",
    "    \n",
    "    # apply AL on specific and general sentence embeddings\n",
    "# make sure to pick the right parameters for uncertainty_sampling and outlier_detection \n",
    "# SIDENOTE: for certainty sampling experiment change max_query to 2200 (for domain 4 even to 3000) for general embeddings - due to different number of positive/negative samples\n",
    "   \n",
    "    \n",
    "    \n",
    "# training the model\n",
    "   # classifier.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n",
    "   # es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "   # checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/classifier/classifier_with_al/random/classifier_domain3_15.h5\")\n",
    "   # history = classifier.fit([np.expand_dims(X_train_gen_all[:pars[0]], 1), np.expand_dims(X_train_spec[:pars[0]], 1)], y_train[:pars[0]], epochs=30, validation_data = ([np.expand_dims(X_val_gen, 1), np.expand_dims(X_val_spec, 1)], y_val), callbacks = [checkpoint, es], batch_size=32)\n",
    "\n",
    "# evaluating the model\n",
    "   # score = classifier.evaluate([np.expand_dims(X_test_gen, 1), np.expand_dims(X_test_spec, 1)], y_test, verbose=0) \n",
    "   # print('Final accuracy score: '+str(score[1]))\n",
    "   # return(i, 'Final accuracy score: '+str(score[1]))\n",
    "\n",
    "\n",
    "\n",
    "# training the model\n",
    "    classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/classifier/classifier_with_al/certainty_sampling/classifier_domain_3.h5\")\n",
    "    history = classifier.fit([np.expand_dims(X_train_gen_al, 1), np.expand_dims(X_train_spec_al, 1)], y_train, epochs=50, validation_data = ([np.expand_dims(X_val_gen, 1), np.expand_dims(X_val_spec, 1)], y_val), callbacks = [checkpoint, es], batch_size=32)\n",
    "\n",
    "# evaluating the model\n",
    "    score = classifier.evaluate([np.expand_dims(X_test_gen, 1), np.expand_dims(X_test_spec, 1)], y_test, verbose=0) \n",
    "    return('Final accuracy score: '+str(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peirama 1 0 (1400, 2000)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pkl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_179/399787402.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpars\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_results_AL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m            \u001b[0;31m# data = pd.DataFrame(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_179/1043992014.py\u001b[0m in \u001b[0;36mreturn_results_AL\u001b[0;34m(i, par0, par1)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreturn_results_AL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpar0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/sentence_embeddings/general/sorted/train/train_data3_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mX_train_gen_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/sentence_embeddings/general/sorted/train/train_labels3_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pkl' is not defined"
     ]
    }
   ],
   "source": [
    "class TestCase:\n",
    "    def __init__(self, name, i_range, pars):\n",
    "        self.name = name\n",
    "        self.i_range = i_range\n",
    "        self.pars = pars\n",
    "        \n",
    "test_cases = [\n",
    "    TestCase(\"peirama 1\", range(0,16), [(1400, 2000)]),\n",
    "#     TestCase(\"peirama 1\", range(0,16), [(1400, 2200), (2100, 3000)]),\n",
    "#     TestCase(\"peirama 2\", range(0,16), [(1400, 2200), (2100, 3000)]),\n",
    "   # TestCase(\"peirama 3\", range(0,100), [(1400, 2200), (2100, 3000)]),\n",
    "]\n",
    "\n",
    "for test_case in test_cases:\n",
    "    for i in test_case.i_range:\n",
    "        for pars in test_case.pars:\n",
    "            print(test_case.name, i, pars)\n",
    "            x = return_results_AL(i, pars[0], pars[1])\n",
    "            print(x)\n",
    "           # data = pd.DataFrame(x)\n",
    "#data.to_excel('sample_data.xlsx', sheet_name='sheet1', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Active Learning and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, a model is trained on 420 random samples as comparison. If a specific active learning algorithm is desired to be executed, this section can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "44/44 [==============================] - 7s 36ms/step - loss: 0.6728 - accuracy: 0.6050 - val_loss: 0.6601 - val_accuracy: 0.6367\n",
      "Epoch 2/30\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.6129 - accuracy: 0.6914 - val_loss: 0.5958 - val_accuracy: 0.7200\n",
      "Epoch 3/30\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.5052 - accuracy: 0.8200 - val_loss: 0.5052 - val_accuracy: 0.7817\n",
      "Epoch 4/30\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.4057 - accuracy: 0.8521 - val_loss: 0.4419 - val_accuracy: 0.8000\n",
      "Epoch 5/30\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.3417 - accuracy: 0.8721 - val_loss: 0.4142 - val_accuracy: 0.8150\n",
      "Epoch 6/30\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.3101 - accuracy: 0.8793 - val_loss: 0.3942 - val_accuracy: 0.8217\n",
      "Epoch 7/30\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.2764 - accuracy: 0.8914 - val_loss: 0.4106 - val_accuracy: 0.8217\n",
      "Epoch 8/30\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.2582 - accuracy: 0.9000 - val_loss: 0.3961 - val_accuracy: 0.8267\n",
      "Epoch 9/30\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.2436 - accuracy: 0.9043 - val_loss: 0.3903 - val_accuracy: 0.8367\n",
      "Epoch 10/30\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.2330 - accuracy: 0.9079 - val_loss: 0.3914 - val_accuracy: 0.8350\n",
      "Epoch 11/30\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.2174 - accuracy: 0.9221 - val_loss: 0.3991 - val_accuracy: 0.8300\n",
      "Epoch 12/30\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.2099 - accuracy: 0.9157 - val_loss: 0.3972 - val_accuracy: 0.8367\n",
      "Epoch 13/30\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.2107 - accuracy: 0.9179 - val_loss: 0.4087 - val_accuracy: 0.8400\n",
      "Epoch 14/30\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1962 - accuracy: 0.9271 - val_loss: 0.4152 - val_accuracy: 0.8433\n",
      "Epoch 15/30\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1898 - accuracy: 0.9250 - val_loss: 0.4102 - val_accuracy: 0.8400\n",
      "Epoch 16/30\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1883 - accuracy: 0.9264 - val_loss: 0.4294 - val_accuracy: 0.8400\n",
      "Epoch 17/30\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1791 - accuracy: 0.9329 - val_loss: 0.4339 - val_accuracy: 0.8417\n",
      "Epoch 18/30\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1805 - accuracy: 0.9336 - val_loss: 0.4356 - val_accuracy: 0.8417\n",
      "Epoch 19/30\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.1691 - accuracy: 0.9364 - val_loss: 0.4367 - val_accuracy: 0.8383\n",
      "Final accuracy score: 0.8491666913032532\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "classifier.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/classifier/classifier_with_al/random/classifier_domain3_15.h5\")\n",
    "history = classifier.fit([np.expand_dims(X_train_gen_all[:1400], 1), np.expand_dims(X_train_spec[:1400], 1)], y_train[:1400], epochs=30, validation_data = ([np.expand_dims(X_val_gen, 1), np.expand_dims(X_val_spec, 1)], y_val), callbacks = [checkpoint, es], batch_size=32)\n",
    "\n",
    "# evaluating the model\n",
    "score = classifier.evaluate([np.expand_dims(X_test_gen, 1), np.expand_dims(X_test_spec, 1)], y_test, verbose=0) \n",
    "print('Final accuracy score: '+str(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells need to be executed for applying active learning and then training the classifier. In order to execute the different experiments, the input parameters of the function \"AL()\" simply need to be adjusted  in the next cell according to the desired configuration. For this please take a look at how the parameters of the function \"AL()\" are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort general sentence embeddings so that general and specific sentence embeddings have the same labels \n",
    "# and number of instances\n",
    "ind = sort_array(y_train_gen_al, y_train_spec_al)\n",
    "X_train_gen_al, y_train = X_train_gen_al[ind], y_train_spec_al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "45/45 [==============================] - 7s 37ms/step - loss: 0.6905 - accuracy: 0.5430 - val_loss: 0.6850 - val_accuracy: 0.5417\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.6836 - accuracy: 0.5563 - val_loss: 0.6764 - val_accuracy: 0.5450\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.6803 - accuracy: 0.5634 - val_loss: 0.6728 - val_accuracy: 0.6067\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.6741 - accuracy: 0.5831 - val_loss: 0.6579 - val_accuracy: 0.6567\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.6669 - accuracy: 0.6239 - val_loss: 0.6461 - val_accuracy: 0.7133\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.6560 - accuracy: 0.6331 - val_loss: 0.6287 - val_accuracy: 0.7217\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.6440 - accuracy: 0.6423 - val_loss: 0.5867 - val_accuracy: 0.7100\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.6297 - accuracy: 0.6570 - val_loss: 0.5878 - val_accuracy: 0.7733\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.6088 - accuracy: 0.6901 - val_loss: 0.5543 - val_accuracy: 0.7583\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.5948 - accuracy: 0.6824 - val_loss: 0.5165 - val_accuracy: 0.8100\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.5819 - accuracy: 0.6979 - val_loss: 0.5180 - val_accuracy: 0.8067\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.5671 - accuracy: 0.7113 - val_loss: 0.5130 - val_accuracy: 0.8117\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.5478 - accuracy: 0.7338 - val_loss: 0.4799 - val_accuracy: 0.8183\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.5447 - accuracy: 0.7042 - val_loss: 0.4841 - val_accuracy: 0.7967\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.5279 - accuracy: 0.7303 - val_loss: 0.5036 - val_accuracy: 0.8333\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.5199 - accuracy: 0.7380 - val_loss: 0.4675 - val_accuracy: 0.8333\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.5030 - accuracy: 0.7542 - val_loss: 0.4777 - val_accuracy: 0.8333\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.5059 - accuracy: 0.7437 - val_loss: 0.4656 - val_accuracy: 0.8333\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.4967 - accuracy: 0.7606 - val_loss: 0.4598 - val_accuracy: 0.8417\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.4776 - accuracy: 0.7683 - val_loss: 0.4477 - val_accuracy: 0.8550\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.4685 - accuracy: 0.7775 - val_loss: 0.4867 - val_accuracy: 0.8383\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.4593 - accuracy: 0.7817 - val_loss: 0.4836 - val_accuracy: 0.8417\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.4656 - accuracy: 0.7641 - val_loss: 0.4493 - val_accuracy: 0.8517\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.4629 - accuracy: 0.7739 - val_loss: 0.5043 - val_accuracy: 0.8233\n",
      "Epoch 25/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.4434 - accuracy: 0.7930 - val_loss: 0.4400 - val_accuracy: 0.8633\n",
      "Epoch 26/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.4643 - accuracy: 0.7704 - val_loss: 0.4704 - val_accuracy: 0.8533\n",
      "Epoch 27/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.4336 - accuracy: 0.7937 - val_loss: 0.4801 - val_accuracy: 0.8433\n",
      "Epoch 28/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.4192 - accuracy: 0.8190 - val_loss: 0.5006 - val_accuracy: 0.8533\n",
      "Epoch 29/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.4188 - accuracy: 0.8077 - val_loss: 0.5044 - val_accuracy: 0.8533\n",
      "Epoch 30/30\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.4237 - accuracy: 0.7944 - val_loss: 0.5543 - val_accuracy: 0.8183\n",
      "Final accuracy score: 0.7788944840431213\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/classifier/classifier_with_al/certainty_sampling/classifier_domain_3.h5\")\n",
    "history = classifier.fit([np.expand_dims(X_train_gen_al, 1), np.expand_dims(X_train_spec_al, 1)], y_train, epochs=30, validation_data = ([np.expand_dims(X_val_gen, 1), np.expand_dims(X_val_spec, 1)], y_val), callbacks = [checkpoint, es], batch_size=32)\n",
    "\n",
    "# evaluating the model\n",
    "score = classifier.evaluate([np.expand_dims(X_test_gen, 1), np.expand_dims(X_test_spec, 1)], y_test, verbose=0) \n",
    "print('Final accuracy score: '+str(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
