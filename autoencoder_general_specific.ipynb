{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following file contains the code for training the autoencoder and generating (and saving) the sentence embeddings. For generating the general sentence embeddings, the autoencoder is trained on all the data and for generating the specific sentence embeddings the autoencoder is trained on a chosen domain. In this work, for the specific sentence embeddings an additional cell is simply executed, this cell is highlighted by the \"CAUTION\" comment. Besides, the batch_size for general sentence embeddings was set to 32 and for specific sentence embeddings to 16 as commented in the corresponding cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the directories \"weights/autoencoder/general/\", \"data/sentence_embeddings/general/unsorted/sentemb/\" and \"data/sentence_embeddings/general/unsorted/label_domain/\" for saving the weights and sentence embeddings when training the autoencoder on all the data. For training the autoencoder on data of one domain exclusively create the directories \"weights/autoencoder/specific/\", \"data/sentence_embeddings/specific/sentemb/\" and \"data/sentence_embeddings/specific/label_domain/\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and setting configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_self_attention import SeqSelfAttention\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up argparse\n",
    "parser = argparse.ArgumentParser(description=\"Your Script Description\")\n",
    "parser.add_argument('--embedding_type', default='GENERAL', help=\"Type of embedding: GENERAL or SPECIFIC\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Define embedding_type as a global variable\n",
    "embedding_type = args.embedding_type\n",
    "\n",
    "SEQUENCE_LEN = 50\n",
    "EMBED_SIZE = 300\n",
    "LATENT_SIZE = 300\n",
    "encoding_dim = 100\n",
    "\n",
    "\n",
    "\n",
    "def load_data_from_file(filename):\n",
    "    \"\"\"Load data from a file.\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pkl.load(f)\n",
    "\n",
    "def shuffle_data(data, labels, seed):\n",
    "    \"\"\"Shuffle data and labels.\"\"\"\n",
    "    idx = np.random.RandomState(seed=seed).permutation(data.shape[0])\n",
    "    return data[idx], labels[:, idx]\n",
    "\n",
    "def filter_data_by_domain(X, label_domain, domain):\n",
    "    \"\"\"Filter data by domain.\"\"\"\n",
    "    index_domain = [i for i, e in enumerate(label_domain[1]) if e == domain]\n",
    "    return X[index_domain], label_domain[:, index_domain]\n",
    "\n",
    "# Build autoencoder\n",
    "def build_autoencoder():\n",
    "    # encoder\n",
    "    inp = tf.keras.Input(shape=(SEQUENCE_LEN, EMBED_SIZE))\n",
    "    enc_out1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=LATENT_SIZE, input_shape=(SEQUENCE_LEN, EMBED_SIZE), return_sequences=True))(inp)\n",
    "    inp_att, attn_weights = SeqSelfAttention(return_attention=True)(enc_out1)\n",
    "    enc_out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=300, input_shape=(SEQUENCE_LEN, EMBED_SIZE)), merge_mode='sum')(inp_att)\n",
    "\n",
    "    # encoder model (to extract sentence embeddings later)\n",
    "    encoder_model = tf.keras.Model(inputs=inp, outputs=enc_out)\n",
    "\n",
    "    rep_vec = tf.keras.layers.RepeatVector(SEQUENCE_LEN)(enc_out)\n",
    "\n",
    "    # decoder\n",
    "    dec_lstm_out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=300, return_sequences=True), merge_mode='sum')(rep_vec)\n",
    "    dec_dense_out = tf.keras.layers.Dense(EMBED_SIZE)(dec_lstm_out)\n",
    "\n",
    "    # entire autoencoder model\n",
    "    autoencoder = tf.keras.Model(inp, dec_dense_out)\n",
    "    return autoencoder\n",
    "\n",
    "def remove_unlabeled_entries(embeddings, label_domain_data):\n",
    "    \"\"\"\n",
    "    Remove unlabeled entries from the embeddings and label_domain_data.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(label_domain_data.transpose(), columns=['label', 'domain', 'idx_domain'])\n",
    "    list_unlabel = df.index[df['label'] == 3].tolist()\n",
    "\n",
    "    # Delete the rows with label=3 (unlabeled)\n",
    "    df_filtered = df[~df.index.isin(list_unlabel)].reset_index(drop=True)\n",
    "    label_domain_filtered = df_filtered.to_numpy().transpose()\n",
    "\n",
    "    df_embeddings = pd.DataFrame(embeddings)\n",
    "    df_embeddings_filtered = df_embeddings[~df_embeddings.index.isin(list_unlabel)].reset_index(drop=True)\n",
    "    embeddings_filtered = df_embeddings_filtered.to_numpy().transpose()\n",
    "\n",
    "    return embeddings_filtered, label_domain_filtered\n",
    "\n",
    "def save_embeddings(embedding_type, embeddings, label_domain_merged, label_domain_test):\n",
    "    \"\"\"\n",
    "    Save the embeddings and label domain data based on the embedding type.\n",
    "    \"\"\"\n",
    "    global embedding_type\n",
    "    if embedding_type == 'GENERAL':\n",
    "        path_prefix = \"data/sentence_embeddings/general/unsorted\"\n",
    "    else:  # SPECIFIC\n",
    "        path_prefix = \"data/sentence_embeddings/specific\"\n",
    "\n",
    "    pkl.dump(embeddings, open(f\"{path_prefix}/sentemb/sentemb_unlabeled14.pkl\", \"wb\"))\n",
    "    pkl.dump(label_domain_merged, open(f\"{path_prefix}/label_domain/label_domain_train_sentemb_unlabeled.pkl\", \"wb\"))\n",
    "    pkl.dump(label_domain_test, open(f\"{path_prefix}/label_domain/label_domain_test_sentemb_unlabeled14.pkl\", \"wb\"))\n",
    "\n",
    "def process_and_save_embeddings(encoder_model, data, label_domain_merged, label_domain_test, embedding_type, domain=None):\n",
    "    \n",
    "    # Use encoder to generate sentence embeddings\n",
    "    sentence_embeddings = encoder_model.predict(data)\n",
    "\n",
    "    # Create DataFrame from label_domain_merged\n",
    "    df_labels = pd.DataFrame(label_domain_merged.transpose(), columns=['label', 'domain', 'idx_domain'])\n",
    "\n",
    "    # Identify and filter out unlabeled rows\n",
    "    list_unlabel = df_labels.index[df_labels['label'] == 3].to_list()\n",
    "    df_labels_filtered = df_labels[~df_labels.index.isin(list_unlabel)].reset_index(drop=True)\n",
    "    df_embeddings_filtered = pd.DataFrame(sentence_embeddings)[~pd.DataFrame(sentence_embeddings).index.isin(list_unlabel)].reset_index(drop=True)\n",
    "\n",
    "    # Convert DataFrames back to numpy arrays\n",
    "    filtered_labels = df_labels_filtered.to_numpy().transpose()\n",
    "    filtered_embeddings = df_embeddings_filtered.to_numpy().transpose()\n",
    "\n",
    "     # Determine save paths based on embedding type\n",
    "    if embedding_type == 'GENERAL':\n",
    "        base_path = \"data/sentence_embeddings/general/unsorted/\"\n",
    "        filename_suffix = \"unlabeled14\"\n",
    "    else:  # SPECIFIC\n",
    "        if domain is None:\n",
    "            raise ValueError(\"For specific embeddings, the domain number must be provided.\")\n",
    "        base_path = \"data/sentence_embeddings/specific/unsorted/\"\n",
    "        filename_suffix = f\"unlabeled{domain}_14\"\n",
    "\n",
    "    # Save the processed embeddings and labels\n",
    "    pkl.dump(filtered_embeddings, open(os.path.join(base_path, f\"sentemb/sentemb_{filename_suffix}.pkl\"), \"wb\"))\n",
    "    pkl.dump(filtered_labels, open(os.path.join(base_path, f\"label_domain/label_domain_train_sentemb_{filename_suffix}.pkl\"), \"wb\"))\n",
    "    pkl.dump(label_domain_test, open(os.path.join(base_path, f\"label_domain/label_domain_test_sentemb_{filename_suffix}.pkl\"), \"wb\"))\n",
    "\n",
    "    return filtered_embeddings, filtered_labels\n",
    "\n",
    "def load_data_from_file(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pkl.load(f)\n",
    "\n",
    "def main():\n",
    "    global embedding_type\n",
    "    # Load the common data for all domains\n",
    "    with h5py.File('data/fully_preprocessed_data/X_merged_preprocessed_new14.h5', 'r') as g:\n",
    "        X_merged = np.zeros((55824, 50, 300), dtype='float64')\n",
    "        g['data'].read_direct(X_merged)\n",
    "\n",
    "    with h5py.File('data/fully_preprocessed_data/X_test_preprocessed_new14.h5', 'r') as h:\n",
    "        X_test = np.zeros((6400, 50, 300), dtype='float64')\n",
    "        h['data'].read_direct(X_test)\n",
    "\n",
    "    # Load domain-specific labels\n",
    "    label_domain_test = load_data_from_file(f'domain_and_label_test.pkl')\n",
    "    label_domain_merged = load_data_from_file(f'domain_and_label_merged.pkl')    \n",
    "\n",
    "    # Determine embedding type\n",
    "    embedding_type = os.environ.get('EMBEDDING_TYPE', 'GENERAL')  # default is 'GENERAL'\n",
    "\n",
    "    if embedding_type == 'SPECIFIC':\n",
    "        domains = range(0, 16)  # Assuming domains are numbered from 1 to 16\n",
    "        for domain in domains:\n",
    "            print(f\"Processing for domain {domain}...\")\n",
    "\n",
    "            # Filter data based on the domain\n",
    "            X_merged_domain, label_domain_merged_domain = filter_data_by_domain(X_merged, label_domain_merged, domain)\n",
    "            X_test_domain, label_domain_test_domain = filter_data_by_domain(X_test, label_domain_test, domain)\n",
    "            # Concatenate train and test data\n",
    "            data_domain = np.concatenate([X_merged_domain, X_test_domain])\n",
    "\n",
    "            # Train and process for the specific domain\n",
    "            autoencoder = build_autoencoder()\n",
    "            autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "            checkpoint_path = f\"weights/autoencoder/specific/autoencoder_weights_with_unlabeled_{domain}.h5\"\n",
    "            checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path)\n",
    "            es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "            history = autoencoder.fit(data_domain, epochs=25, callbacks=[checkpoint, es], batch_size=16)\n",
    "            processed_embeddings, processed_labels = process_and_save_embeddings(autoencoder, data_domain, label_domain_merged_domain, label_domain_test_domain, embedding_type)\n",
    "    \n",
    "\n",
    "    else:  # For 'GENERAL'\n",
    "        # Load domain-specific labels for GENERAL type\n",
    "        label_domain_test = load_data_from_file('domain_and_label_test.p')\n",
    "        label_domain_merged = load_data_from_file('domain_and_label_merged.p')\n",
    "        # Shuffle data\n",
    "        X_merged, label_domain_merged = shuffle_data(X_merged, label_domain_merged, seed=42)\n",
    "        X_test, label_domain_test = shuffle_data(X_test, label_domain_test, seed=43)\n",
    "\n",
    "        # Concatenate train and test data\n",
    "        data = np.concatenate([X_merged, X_test])\n",
    "\n",
    "        # Train and process for general embeddings\n",
    "        autoencoder = build_autoencoder()\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        checkpoint_path = \"weights/autoencoder/general/autoencoder_weights_with_unlabeled.h5\"\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path)\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "        history = autoencoder.fit(data, data, epochs=25, callbacks=[checkpoint, es], batch_size=32)\n",
    "        processed_embeddings, processed_labels = process_and_save_embeddings(autoencoder, data, label_domain_merged, label_domain_test, embedding_type, domain=domain)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
