{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following file contains the code for training the autoencoder and generating (and saving) the sentence embeddings. For generating the general sentence embeddings, the autoencoder is trained on all the data and for generating the specific sentence embeddings the autoencoder is trained on a chosen domain. In this work, for the specific sentence embeddings an additional cell is simply executed, this cell is highlighted by the \"CAUTION\" comment. Besides, the batch_size for general sentence embeddings was set to 32 and for specific sentence embeddings to 16 as commented in the corresponding cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the directories \"weights/autoencoder/general/\", \"data/sentence_embeddings/general/unsorted/sentemb/\" and \"data/sentence_embeddings/general/unsorted/label_domain/\" for saving the weights and sentence embeddings when training the autoencoder on all the data. For training the autoencoder on data of one domain exclusively create the directories \"weights/autoencoder/specific/\", \"data/sentence_embeddings/specific/sentemb/\" and \"data/sentence_embeddings/specific/label_domain/\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and setting configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_self_attention import SeqSelfAttention\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 50\n",
    "EMBED_SIZE = 300\n",
    "LATENT_SIZE = 300\n",
    "encoding_dim = 100\n",
    "\n",
    "def load_data_from_file(filename):\n",
    "    \"\"\"Load data from a file.\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pkl.load(f)\n",
    "\n",
    "def shuffle_data(data, labels, seed):\n",
    "    \"\"\"Shuffle data and labels.\"\"\"\n",
    "    idx = np.random.RandomState(seed=seed).permutation(data.shape[0])\n",
    "    return data[idx], labels[:, idx]\n",
    "\n",
    "def filter_data_by_domain(X, label_domain, domain):\n",
    "    \"\"\"Filter data by domain.\"\"\"\n",
    "    index_domain = [i for i, e in enumerate(label_domain[1]) if e == domain]\n",
    "    return X[index_domain], label_domain[:, index_domain]\n",
    "\n",
    "# Build autoencoder\n",
    "def build_autoencoder():\n",
    "    # encoder\n",
    "    inp = tf.keras.Input(shape=(SEQUENCE_LEN, EMBED_SIZE))\n",
    "    enc_out1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=LATENT_SIZE, input_shape=(SEQUENCE_LEN, EMBED_SIZE), return_sequences=True))(inp)\n",
    "    inp_att, attn_weights = SeqSelfAttention(return_attention=True)(enc_out1)\n",
    "    enc_out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=300, input_shape=(SEQUENCE_LEN, EMBED_SIZE)), merge_mode='sum')(inp_att)\n",
    "\n",
    "    # encoder model (to extract sentence embeddings later)\n",
    "    encoder_model = tf.keras.Model(inputs=inp, outputs=enc_out)\n",
    "\n",
    "    rep_vec = tf.keras.layers.RepeatVector(SEQUENCE_LEN)(enc_out)\n",
    "\n",
    "    # decoder\n",
    "    dec_lstm_out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=300, return_sequences=True), merge_mode='sum')(rep_vec)\n",
    "    dec_dense_out = tf.keras.layers.Dense(EMBED_SIZE)(dec_lstm_out)\n",
    "\n",
    "    # entire autoencoder model\n",
    "    autoencoder = tf.keras.Model(inp, dec_dense_out)\n",
    "    return autoencoder\n",
    "\n",
    "def remove_unlabeled_entries(embeddings, label_domain_data):\n",
    "    \"\"\"\n",
    "    Remove unlabeled entries from the embeddings and label_domain_data.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(label_domain_data.transpose(), columns=['label', 'domain', 'idx_domain'])\n",
    "    list_unlabel = df.index[df['label'] == 3].tolist()\n",
    "\n",
    "    # Delete the rows with label=3 (unlabeled)\n",
    "    df_filtered = df[~df.index.isin(list_unlabel)].reset_index(drop=True)\n",
    "    label_domain_filtered = df_filtered.to_numpy().transpose()\n",
    "\n",
    "    df_embeddings = pd.DataFrame(embeddings)\n",
    "    df_embeddings_filtered = df_embeddings[~df_embeddings.index.isin(list_unlabel)].reset_index(drop=True)\n",
    "    embeddings_filtered = df_embeddings_filtered.to_numpy().transpose()\n",
    "\n",
    "    return embeddings_filtered, label_domain_filtered\n",
    "\n",
    "def save_embeddings(embedding_type, embeddings, label_domain_merged, label_domain_test):\n",
    "    \"\"\"\n",
    "    Save the embeddings and label domain data based on the embedding type.\n",
    "    \"\"\"\n",
    "    if embedding_type == 'GENERAL':\n",
    "        path_prefix = \"data/sentence_embeddings/general/unsorted\"\n",
    "    else:  # SPECIFIC\n",
    "        path_prefix = \"data/sentence_embeddings/specific\"\n",
    "\n",
    "    pkl.dump(embeddings, open(f\"{path_prefix}/sentemb/sentemb_unlabeled14.p\", \"wb\"))\n",
    "    pkl.dump(label_domain_merged, open(f\"{path_prefix}/label_domain/label_domain_train_sentemb_unlabeled14.p\", \"wb\"))\n",
    "    pkl.dump(label_domain_test, open(f\"{path_prefix}/label_domain/label_domain_test_sentemb_unlabeled14.p\", \"wb\"))\n",
    "\n",
    "def process_and_save_embeddings(encoder_model, data, label_domain_merged, label_domain_test, embedding_type):\n",
    "    # Use encoder to generate sentence embeddings\n",
    "    sentence_embeddings = encoder_model.predict(data)\n",
    "\n",
    "    # Create DataFrame from label_domain_merged\n",
    "    df_labels = pd.DataFrame(label_domain_merged.transpose(), columns=['label', 'domain', 'idx_domain'])\n",
    "\n",
    "    # Identify and filter out unlabeled rows\n",
    "    list_unlabel = df_labels.index[df_labels['label'] == 3].to_list()\n",
    "    df_labels_filtered = df_labels[~df_labels.index.isin(list_unlabel)].reset_index(drop=True)\n",
    "    df_embeddings_filtered = pd.DataFrame(sentence_embeddings)[~pd.DataFrame(sentence_embeddings).index.isin(list_unlabel)].reset_index(drop=True)\n",
    "\n",
    "    # Convert DataFrames back to numpy arrays\n",
    "    filtered_labels = df_labels_filtered.to_numpy().transpose()\n",
    "    filtered_embeddings = df_embeddings_filtered.to_numpy().transpose()\n",
    "\n",
    "    # Determine save paths based on embedding type\n",
    "    if embedding_type == 'GENERAL':\n",
    "        base_path = \"data/sentence_embeddings/general/unsorted/\"\n",
    "    else:  # SPECIFIC\n",
    "        base_path = \"data/sentence_embeddings/specific/unsorted/\"\n",
    "\n",
    "    # Save the processed embeddings and labels\n",
    "    pkl.dump(filtered_embeddings, open(os.path.join(base_path, f\"sentemb/sentemb_unlabeled14.p\"), \"wb\"))\n",
    "    pkl.dump(filtered_labels, open(os.path.join(base_path, f\"label_domain/label_domain_train_sentemb_unlabeled14.p\"), \"wb\"))\n",
    "    pkl.dump(label_domain_test, open(os.path.join(base_path, f\"label_domain/label_domain_test_sentemb_unlabeled14.p\"), \"wb\"))\n",
    "\n",
    "    return filtered_embeddings, filtered_labels\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    label_domain_test = load_data_from_file('domain_and_label_test14.p')\n",
    "    label_domain_merged = load_data_from_file('domain_and_label_merged14.p')\n",
    "\n",
    "    # Shuffle data\n",
    "    X_merged, label_domain_merged = shuffle_data(X_merged, label_domain_merged, seed=42)\n",
    "    X_test, label_domain_test = shuffle_data(X_test, label_domain_test, seed=43)\n",
    "\n",
    "    # Check environment variable and decide training type\n",
    "    embedding_type = os.environ.get('EMBEDDING_TYPE', 'GENERAL')  # default is 'GENERAL'\n",
    "    domain = int(os.environ.get('DOMAIN_NUMBER', 5))  # default is 5\n",
    "\n",
    "    # Filter data based on domain if embedding type is SPECIFIC\n",
    "    if embedding_type == 'SPECIFIC':\n",
    "        X_merged, label_domain_merged = filter_data_by_domain(X_merged, label_domain_merged, domain)\n",
    "        X_test, label_domain_test = filter_data_by_domain(X_test, label_domain_test, domain)\n",
    "\n",
    "    # Concatenate train and test data\n",
    "    data = np.concatenate([X_merged, X_test])\n",
    "\n",
    "    # Build and compile the autoencoder model\n",
    "    autoencoder = build_autoencoder()\n",
    "    autoencoder.summary()\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Callbacks\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    \n",
    "    # Set paths and batch sizes depending on embedding type\n",
    "    if embedding_type == 'GENERAL':\n",
    "        checkpoint_path = \"weights/autoencoder/general/autoencoder_weights_with_unlabeled14.h5\"\n",
    "        batch_size = 32\n",
    "    else:  # SPECIFIC\n",
    "        checkpoint_path = \"weights/autoencoder/specific/autoencoder_weights_with_unlabeled4_5.h5\"\n",
    "        batch_size = 16\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path)\n",
    "\n",
    "    # Train the model\n",
    "    history = autoencoder.fit(data, data, epochs=25, callbacks=[checkpoint, es], batch_size=batch_size)\n",
    "\n",
    "    # Check environment variables\n",
    "    embedding_type = os.environ.get('EMBEDDING_TYPE', 'GENERAL')  # default is 'GENERAL'\n",
    "\n",
    "    # Process and save based on the embedding type\n",
    "    processed_embeddings, processed_labels = process_and_save_embeddings(encoder_model, data, label_domain_merged, label_domain_test, embedding_type)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the encoder to generate the sentence embeddings\n",
    "sentence_embeddings_gen = encoder_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the encoder to generate the sentence embeddings\n",
    "sentence_embeddings_spec = encoder_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62224, 300)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings_gen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2000)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1599)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_domain_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process for the general Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(label_domain_merged.transpose(), columns = ['label','domain','idx_domain'])\n",
    "\n",
    "df_merged = pd.DataFrame(sentence_embeddings_gen)\n",
    "\n",
    "df = pd.DataFrame(label_domain_merged.transpose(), columns = ['label','domain','idx_domain'])\n",
    "\n",
    "list_unlabel = df.index[df['label'] == 3].to_list()\n",
    "\n",
    "\n",
    "#Delete the rows with label=3 (unlabeled) in the label_merged\n",
    "df_label_merged = df[~df.index.isin(list_unlabel)].reset_index(drop=True)\n",
    "\n",
    "array = df_label_merged.to_numpy()\n",
    "label_domain_merged = array.transpose()\n",
    "\n",
    "df_merged2 = df_merged[~df_merged.index.isin(list_unlabel)].reset_index(drop=True)\n",
    "\n",
    "array2 = df_merged2.to_numpy()\n",
    "sentence_embeddings = array2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 31780)"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process for Specific Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_labels_spec = pd.DataFrame(label_domain_merged.transpose(), columns = ['label','domain','idx_domain'])\n",
    "\n",
    "df_merged_spec = pd.DataFrame(sentence_embeddings_spec)\n",
    "\n",
    "list_unlabel_spec = df_labels_spec.index[df_labels_spec['label'] == 3].to_list()\n",
    "\n",
    "df_labels_spec2 = df_labels_spec[~df_labels_spec.index.isin(list_unlabel_spec)].reset_index(drop=True)\n",
    "\n",
    "array = df_labels_spec2.to_numpy()\n",
    "label_domain_merged = array.transpose()\n",
    "\n",
    "df_merged_spec2 = df_merged_spec[~df_merged_spec.index.isin(list_unlabel_spec)].reset_index(drop=True)\n",
    "\n",
    "array2 = df_merged_spec2.to_numpy()\n",
    "sentence_embeddings_spec = array2.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Sentences Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sentence embeddings as well as the shuffled labels\n",
    "# here the general embeddings were saved in the general embeddings folder\n",
    "# change directories to \"data/sentence_embeddings/specific/sentemb/\" and \"data/sentence_embeddings/specific/label_domain/\" for specific sentence embeddings\n",
    "pkl.dump(sentence_embeddings, open(\"data/sentence_embeddings/general/unsorted/sentemb/sentemb_unlabeled14.p\", \"wb\"))\n",
    "pkl.dump(label_domain_merged, open(\"data/sentence_embeddings/general/unsorted/label_domain/label_domain_train_sentemb_unlabeled14.p\", \"wb\"))\n",
    "pkl.dump(label_domain_test, open(\"data/sentence_embeddings/general/unsorted/label_domain/label_domain_test_sentemb_unlabeled14.p\", \"wb\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific Sentences Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sentence embeddings as well as the shuffled labels\n",
    "# here the general embeddings were saved in the general embeddings folder\n",
    "# change directories to \"data/sentence_embeddings/specific/sentemb/\" and \"data/sentence_embeddings/specific/label_domain/\" for specific sentence embeddings\n",
    "pkl.dump(sentence_embeddings_spec, open(\"data/sentence_embeddings/specific/sentemb/sentemb_unlabeled4_5.p\", \"wb\"))\n",
    "pkl.dump(label_domain_merged, open(\"data/sentence_embeddings/specific/label_domain/label_domain_train_sentemb_unlabeled4_5.p\", \"wb\"))\n",
    "pkl.dump(label_domain_test, open(\"data/sentence_embeddings/specific/label_domain/label_domain_test_sentemb_unlabeled4_5.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding preprocessed data into an already trained Autoencoder to predict sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general sentence embeddings\n",
    "# data needs to be the full data without the cell on top of this file being executed for selecting a domain\n",
    "autoencoder.load_weights('weights/autoencoder/general/autoencoder_weights_with_unlabeled4.h5')\n",
    "sentence_embeddings_general = encoder_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific sentence embeddings\n",
    "# data needs to be of only one domain, the cell for selecting the domain on top of the file needs to be executed\n",
    "# here domain 0 was chosen (MR) as an example\n",
    "autoencoder.load_weights('weights/autoencoder/specific/autoencoder_weights_with_unlabeled4_1.h5')\n",
    "#sentence_embeddings_specific_1 = encoder_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62169, 300)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings_general.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1970)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
