{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained BERT model needs to be downloaded under https://github.com/hanxiao/bert-as-service. This work uses \"BERT Base, Uncased\" as a BERT model. After the download, a folder called \"uncased_L-12_H-768_A-12\" is created which has to be saved under the relative path \"data/bert_model/\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a new service needs to be started. For this, you have to open a new command prompt, go into the directory of this file and then execute the following command: \n",
    "\n",
    "bert-serving-start -num_worker=1 -model_dir=data/bert_model/uncased_L-12_H-768_A-12/ -max_seq_len=512\n",
    "\n",
    "The number of workers can be chosen accordingly. In this work just one worker was chosen. The maximum sequence length was set to 512 which is the number of maximum tokens of the original BERT model. Sequences that are too long will then be cut off on the right side. No special truncation has been chosen here. The reason for this is that it has already been shown that the proportion of reviews longer than 100 tokens is rather little, therefore the number of reviews longer than 512 tokens is assumed to be negligibly small.\n",
    "\n",
    "After executing the above command, a message called \"all set, ready to serve request!\" is returned in the command prompt. This notebook can then be executed in order to generate sentence embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to create a directory \"data/bert_embeddings/train/\" and \"data/bert_embeddings/test/\" such that the train/test data can be saved there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from bert_serving.client import BertClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the data is cleaned and then encoded to sentence embeddings. The reason for cleaning the data again is that creating the BERT embeddings is a rather time-consuming matter. By cleaning the data file wise and then encoding the reviews, it's easier to feed portions into BERT and divide the process of generating sentence embeddings. The sentence embeddings are saved in the same file structure as the raw data files as well (per domain two files: train and test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the stopwords list\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords_keep =['no', 'not', 'nor']\n",
    "stopwords = list(set(stopwords).difference(set(stopwords_keep)))\n",
    "\n",
    "# set the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning function\n",
    "def data_cleaning(df):\n",
    "    \n",
    "    # exchange some incomplete words (due to contractions and opening them up some words are incomplete)\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(['can' if word == 'ca' else word for word in str(x).split()]))\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(['will' if word == 'wo' else word for word in str(x).split()]))\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(['shall' if word == 'sha' else word for word in str(x).split()]))\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(['not' if word == 'nt' or word == \"n't\" else word for word in str(x).split()]))\n",
    "    \n",
    "    # remove punctuation & special characters\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(re.split('\\W+', x)))\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(word for word in x.split() if word.isalnum()))\n",
    "\n",
    "    # remove nouns and numbers\n",
    "    df['text'] = df['text'].astype(str).apply(lambda x: nltk.tag.pos_tag(x.split()))\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([word for word, tag in x if tag != 'NN' and tag != 'NNS' and tag != 'CD']))\n",
    " \n",
    "    # remove stopwords\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))\n",
    "    \n",
    "    # lemmatize\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the data path and output path (take care that both train and test data get encoded)\n",
    "data_path = 'data/uncleaned_data/train/*'\n",
    "output_path = 'data/bert_embeddings/train/'\n",
    "\n",
    "# start bert client\n",
    "bc = BertClient()\n",
    "\n",
    "# open each file in the directory data_path\n",
    "for file_name in glob.glob(data_path):\n",
    "\n",
    "    # read and clean the data\n",
    "    df = pd.read_csv(file_name, delimiter = '\\t', names=[\"label\",\"text\"], encoding='latin-1')\n",
    "    df = data_cleaning(df)\n",
    "    df['text'].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    \n",
    "    # encode and prepare final df\n",
    "    sentence_list = [s for s in df['text']]\n",
    "    embeddings = bc.encode(sentence_list)\n",
    "    df['embeddings'] = pd.DataFrame(zip(embeddings), columns=[\"embeddings\"])\n",
    "    df = df.drop('text', axis = 1)\n",
    "\n",
    "    # save to file\n",
    "    with open(output_path + os.path.basename(file_name) + '.pkl', 'wb') as f:\n",
    "        pkl.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
