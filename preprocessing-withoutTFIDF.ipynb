{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following file contains the code for preprocessing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to create a directory \"data/cleaned_data/\" and \"data/fully_preprocessed_data/\" such that the data can be saved there. Besides, save the fasttext file of https://fasttext.cc/docs/en/english-vectors.html (choosing wiki news - 1 million word vectors) under \"data/fasttext_file/wiki-news-300d-1M.vec\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "# deprecation warning might appear for collections library, however meaningless for python version 3.7.7\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import nltk\n",
    "import h5py\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the stopwords list\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords_keep =['no', 'not', 'nor']\n",
    "stopwords = list(set(stopwords).difference(set(stopwords_keep)))\n",
    "\n",
    "# set the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for data cleaning\n",
    "def data_cleaning(df):\n",
    "    \n",
    "    # handle contracted words\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(['can' if word == 'ca' else word for word in str(x).split()]))\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(['will' if word == 'wo' else word for word in str(x).split()]))\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(['shall' if word == 'sha' else word for word in str(x).split()]))\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(['not' if word == 'nt' or word == \"n't\" else word for word in str(x).split()]))\n",
    "    \n",
    "    # remove punctuation & special characters\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(re.split('\\W+', x)))\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(word for word in x.split() if word.isalnum()))\n",
    "   \n",
    "    # remove nouns and numbers\n",
    "    df['text'] = df['text'].astype(str).apply(lambda x: nltk.tag.pos_tag(x.split()))\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([word for word, tag in x if tag != 'CD']))\n",
    "    # remove stopwords\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))\n",
    "    \n",
    "    # lemmatize\n",
    "   # df['text'] = df['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading and cleaning the data using the data_cleaning() function\n",
    "def load_and_clean(path):\n",
    "    \n",
    "    # load the data into one data frame, create a domain columns that indicates the domain by using the filename\n",
    "    df_list = []\n",
    "    for file_name in glob.glob(path):\n",
    "        df_temp = pd.read_csv(file_name, delimiter = '\\t', names=[\"label\",\"text\"], encoding='latin-1')\n",
    "        df_temp['domain'] = os.path.basename(file_name)\n",
    "        df_list.append(df_temp)    \n",
    "    df = pd.concat(df_list, ignore_index = True)\n",
    "    \n",
    "    # clean the data\n",
    "    df = data_cleaning(df)\n",
    "    \n",
    "    # delete empty strings\n",
    "    df['text'].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading and cleaning the data using the data_cleaning() function\n",
    "def load_and_clean2(path):\n",
    "    \n",
    "    # load the data into one data frame, create a domain columns that indicates the domain by using the filename\n",
    "    df_list = []\n",
    "    for file_name in glob.glob(path):\n",
    "        df_temp = pd.read_csv(file_name, delimiter = '\\t', names=[\"text\"], encoding='latin-1')\n",
    "        df_temp['domain'] = os.path.basename(file_name)\n",
    "        df_list.append(df_temp)    \n",
    "    df = pd.concat(df_list, ignore_index = True)\n",
    "    \n",
    "    # clean the data\n",
    "    df = data_cleaning(df)\n",
    "    \n",
    "    # delete empty strings\n",
    "    df['text'].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                               text  \\\n",
      "0          1  bourne ultimatum review thrilling set get fina...   \n",
      "1          1  hollow man starts brilliant flawed scientist d...   \n",
      "2          1  since title english imdb lists show primary la...   \n",
      "3          0  oh dear englishman small part welsh fan anthon...   \n",
      "4          1  following sitcom plot mindlessly easy characte...   \n",
      "...      ...                                                ...   \n",
      "25375      1  recived paint quickly great condition bad ball...   \n",
      "25376      1  originally bought blue lb red lb black lb grip...   \n",
      "25377      0  rarely get taken time song played times no ins...   \n",
      "25378      1  bought neighbors little girl loved easy put to...   \n",
      "25379      0  well first ordered product amazon sent somethi...   \n",
      "\n",
      "                           domain  \n",
      "0                 imdb.task.train  \n",
      "1                 imdb.task.train  \n",
      "2                 imdb.task.train  \n",
      "3                 imdb.task.train  \n",
      "4                 imdb.task.train  \n",
      "...                           ...  \n",
      "25375  sports_outdoors.task.train  \n",
      "25376  sports_outdoors.task.train  \n",
      "25377  sports_outdoors.task.train  \n",
      "25378  sports_outdoors.task.train  \n",
      "25379  sports_outdoors.task.train  \n",
      "\n",
      "[25380 rows x 3 columns]\n",
      "      label                                               text  \\\n",
      "0         0                    received paper weeks stop comin   \n",
      "1         1  imagine making beehive cake honey citrus iced ...   \n",
      "2         1  magazines writing serious writers need read po...   \n",
      "3         1  although yet subscribe aperture heard little s...   \n",
      "4         1  absolutely love magazine wonderful motivation ...   \n",
      "...     ...                                                ...   \n",
      "6395      1  purchased hair dryer unusual reason old watt c...   \n",
      "6396      0  bought scale figured price right well say get ...   \n",
      "6397      1     good product helps prolong life electric razor   \n",
      "6398      0  not know people talking energy drink tastes li...   \n",
      "6399      1  stuff awesome hangovers like high school stapl...   \n",
      "\n",
      "                              domain  \n",
      "0                magazines.task.test  \n",
      "1                magazines.task.test  \n",
      "2                magazines.task.test  \n",
      "3                magazines.task.test  \n",
      "4                magazines.task.test  \n",
      "...                              ...  \n",
      "6395  health_personal_care.task.test  \n",
      "6396  health_personal_care.task.test  \n",
      "6397  health_personal_care.task.test  \n",
      "6398  health_personal_care.task.test  \n",
      "6399  health_personal_care.task.test  \n",
      "\n",
      "[6400 rows x 3 columns]\n",
      "                                                    text             domain\n",
      "0      zoe clarke williams lackluster thriller new be...    MR.task.unlabel\n",
      "1      macdowell gives give solid anguished performan...    MR.task.unlabel\n",
      "2      powerful telling story examines forbidden love...    MR.task.unlabel\n",
      "3                              allen stopped challenging    MR.task.unlabel\n",
      "4      stars may college kids subject matter adult ge...    MR.task.unlabel\n",
      "...                                                  ...                ...\n",
      "30439  After reading reviews nearly every wipe warmer...  baby.task.unlabel\n",
      "30440  LAs bases para mantener un mamon esteril son f...  baby.task.unlabel\n",
      "30441  I used bottles first son wonderful They never ...  baby.task.unlabel\n",
      "30442  Okay first I get babiesrus wanted charge arm l...  baby.task.unlabel\n",
      "30443  We came back vacation holidays used TOTEaTOT f...  baby.task.unlabel\n",
      "\n",
      "[30444 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# load and clean the train and test data\n",
    "df_train = load_and_clean('data/uncleaned_data/train/*')\n",
    "df_test = load_and_clean('data/uncleaned_data/test/*')\n",
    "df_unlabel = load_and_clean2('data/uncleaned_data/unlabelled/*')\n",
    "# delete \"task.train\" or \"task.text\" such that the column domain just shows the domain\n",
    "#df_train['domain'] = [re.sub('.task.train$', '', word) for word in np.array(df_train['domain'])]\n",
    "#df_test['domain'] = [re.sub('.task.test$', '', word) for word in np.array(df_test['domain'])]\n",
    "#df_unlabel['domain'] = [re.sub('.task.unlabel$', '', word) for word in np.array(df_unlabel['domain'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['domain'] = [re.sub('.task.train$', '', word) for word in np.array(df_train['domain'])]\n",
    "df_test['domain'] = [re.sub('.task.test$', '', word) for word in np.array(df_test['domain'])]\n",
    "df_unlabel['domain'] = [re.sub('.task.unlabel$', '', word) for word in np.array(df_unlabel['domain'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabel[\"label\"]=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train2 = df_train[[\"text\", \"domain\"]]\n",
    "#df_train[\"label_id\"]=1\n",
    "#df_unlabel[\"label_id\"]=0\n",
    "df_merged = pd.concat([df_train, df_unlabel], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25380, 4)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 4)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data to files\n",
    "pkl.dump(df_train, open(\"data/cleaned_data/train_cleaned.p\", \"wb\"))\n",
    "pkl.dump(df_test, open(\"data/cleaned_data/test_cleaned.p\", \"wb\"))\n",
    "pkl.dump(df_merged, open(\"data/cleaned_data/merged_cleaned.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the sequence length and vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of the code below in this file originates from the following book:\n",
    "# Gulli, Antonio, Amita Kapoor, and Sujit Pal. Deep Learning with TensorFlow 2 and Keras: Regression, ConvNets, GANs, RNNs, NLP, and More with TensorFlow 2 and the Keras API. Packt Publishing, Limited, 2019, pp. 365-373."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open cleaned train and test data (if the previous cells were executed a different time)\n",
    "with open('data/cleaned_data/train_cleaned.p', 'rb') as f:\n",
    "    df_train = pkl.load(f)\n",
    "with open('data/cleaned_data/test_cleaned.p', 'rb') as f:\n",
    "    df_test = pkl.load(f)\n",
    "with open('data/cleaned_data/merged_cleaned.p', 'rb') as f:\n",
    "    df_merged = pkl.load(f)\n",
    "    \n",
    "# loading the unknown token for future use\n",
    "with open('unk_token.p', 'rb') as f:\n",
    "    unk = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>domain</th>\n",
       "      <th>idx_domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bourne ultimatum 2007 review thrilling set two...</td>\n",
       "      <td>imdb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hollow man starts brilliant flawed scientist d...</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>since title english imdb lists show primary la...</td>\n",
       "      <td>imdb</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>oh dear englishman small part welsh fan anthon...</td>\n",
       "      <td>imdb</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>following sitcom plot mindlessly easy characte...</td>\n",
       "      <td>imdb</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55819</th>\n",
       "      <td>3</td>\n",
       "      <td>After reading reviews nearly every wipe warmer...</td>\n",
       "      <td>baby</td>\n",
       "      <td>3495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55820</th>\n",
       "      <td>3</td>\n",
       "      <td>LAs bases para mantener un mamon esteril son f...</td>\n",
       "      <td>baby</td>\n",
       "      <td>3496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55821</th>\n",
       "      <td>3</td>\n",
       "      <td>I used bottles first son wonderful They never ...</td>\n",
       "      <td>baby</td>\n",
       "      <td>3497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55822</th>\n",
       "      <td>3</td>\n",
       "      <td>Okay first I get babiesrus wanted charge arm l...</td>\n",
       "      <td>baby</td>\n",
       "      <td>3498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55823</th>\n",
       "      <td>3</td>\n",
       "      <td>We came back vacation holidays used TOTEaTOT f...</td>\n",
       "      <td>baby</td>\n",
       "      <td>3499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55824 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text domain  \\\n",
       "0          1  bourne ultimatum 2007 review thrilling set two...   imdb   \n",
       "1          1  hollow man starts brilliant flawed scientist d...   imdb   \n",
       "2          1  since title english imdb lists show primary la...   imdb   \n",
       "3          0  oh dear englishman small part welsh fan anthon...   imdb   \n",
       "4          1  following sitcom plot mindlessly easy characte...   imdb   \n",
       "...      ...                                                ...    ...   \n",
       "55819      3  After reading reviews nearly every wipe warmer...   baby   \n",
       "55820      3  LAs bases para mantener un mamon esteril son f...   baby   \n",
       "55821      3  I used bottles first son wonderful They never ...   baby   \n",
       "55822      3  Okay first I get babiesrus wanted charge arm l...   baby   \n",
       "55823      3  We came back vacation holidays used TOTEaTOT f...   baby   \n",
       "\n",
       "       idx_domain  \n",
       "0               0  \n",
       "1               1  \n",
       "2               2  \n",
       "3               3  \n",
       "4               4  \n",
       "...           ...  \n",
       "55819        3495  \n",
       "55820        3496  \n",
       "55821        3497  \n",
       "55822        3498  \n",
       "55823        3499  \n",
       "\n",
       "[55824 rows x 4 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MR': 0,\n",
       " 'apparel': 1,\n",
       " 'baby': 2,\n",
       " 'books': 3,\n",
       " 'camera_photo': 4,\n",
       " 'dvd': 5,\n",
       " 'electronics': 6,\n",
       " 'health_personal_care': 7,\n",
       " 'imdb': 8,\n",
       " 'kitchen_housewares': 9,\n",
       " 'magazines': 10,\n",
       " 'music': 11,\n",
       " 'software': 12,\n",
       " 'sports_outdoors': 13,\n",
       " 'toys_games': 14,\n",
       " 'video': 15}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the data: nummerate instances (to know which instances were the orginal ones after shuffling before the autoencoder) \n",
    "# and create a dictionary that numerically encodes the domain \n",
    "\n",
    "# nummerate each instance for each domain\n",
    "df_train['idx_domain'] = df_train.groupby(['domain']).cumcount()\n",
    "df_test['idx_domain'] = df_test.groupby(['domain']).cumcount()\n",
    "df_merged['idx_domain'] = df_merged.groupby('domain').cumcount()\n",
    "# create a dictionary that numerically encodes the domain\n",
    "dict_domain = dict(zip(sorted(set(np.array(df_train['domain']))), np.arange(16)))\n",
    "#dict_domain = dict(zip(sorted(set(np.array(df_merged['domain']))), np.arange(16)))\n",
    "display(dict_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"idx_domain\"]= df_merged[\"idx_domain\"].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divides the data of the data frame into reviews, labels, domain and instance number\n",
    "def divide_data(df, dict_domain):\n",
    "    \n",
    "    reviews = np.array([s for s in df['text']])\n",
    "    X ,y, domain, idx_domain = reviews, np.array(df['label']), np.array(df['domain']), np.array(df['idx_domain'])\n",
    "    domain = [dict_domain.get(c, c) for c in domain]\n",
    "       \n",
    "    return X, y, domain, idx_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divides the data of the data frame into reviews, labels, domain and instance number\n",
    "def divide_data2(df, dict_domain):\n",
    "    \n",
    "    reviews = np.array([s for s in df['text']])\n",
    "    X, y, domain, idx_domain= reviews, np.array(df['label']), np.array(df['domain']), np.array(df['idx_domain'])\n",
    "    domain = [dict_domain.get(c, c) for c in domain]\n",
    "       \n",
    "    return X, y, domain, idx_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide train and test data such that the reviews, labels, domain and instance number are separated\n",
    "X_train, y_train, domain_train, idx_domain_train = divide_data(df_train, dict_domain)\n",
    "X_test, y_test, domain_test, idx_domain_test = divide_data(df_test, dict_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_merged, y_merged, domain_merged, idx_domain_merged= divide_data(df_merged, dict_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse training instances into one list called parsed_reviews_train\n",
    "# and count the number of words per review in the list nr_words_train (needed when determining the vocabulary size and sequence length)\n",
    "\n",
    "word_freqs_merged = collections.Counter()\n",
    "nr_words_merged = []\n",
    "parsed_reviews_merged = []\n",
    "\n",
    "# loop over the different reviews and tokenize the words\n",
    "for review in X_merged:\n",
    "    words = nltk.word_tokenize(review)\n",
    "    parsed_words_merged = []\n",
    "    \n",
    "    # for each review count word frequencies and parse words\n",
    "    for word in words:\n",
    "        word_freqs_merged[word] += 1\n",
    "        parsed_words_merged.append(word)\n",
    "      \n",
    "    # append the number of words in each review, append the parsed reviews\n",
    "    nr_words_merged.append(len(words))      \n",
    "    parsed_reviews_merged.append(\" \".join(parsed_words_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55824"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parsed_reviews_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse test instances into one list called parsed_reviews_test\n",
    "# and count the number of words per review into the list nr_words_esz\n",
    "parsed_reviews_test = []\n",
    "nr_words_test = []\n",
    "for review in X_test:\n",
    "    words = nltk.word_tokenize(review)\n",
    "    parsed_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        parsed_words.append(word)\n",
    "        \n",
    "    nr_words_test.append(len(words))  \n",
    "    parsed_reviews_test.append(\" \".join(parsed_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews longer than 80 words : 12704\n",
      "Reviews in total: 55824\n",
      "Percentage of reviews with more than 80 words: 18.20578962453425 %\n"
     ]
    }
   ],
   "source": [
    "# determining the sequence length\n",
    "\n",
    "# count the reviews that are longer than 100 words\n",
    "nr_reviews_merged_long = sum(i > 80 for i in nr_words_merged)\n",
    "\n",
    "# create a histogram\n",
    "plt.hist(nr_words_merged, bins = 35, range = (0,400))\n",
    "plt.xlabel('\\nNumber of words in a review\\n', fontsize = 10)\n",
    "plt.ylabel('\\nNumber of reviews\\n', fontsize = 10)\n",
    "plt.show()\n",
    "\n",
    "# print calculated numbers for longer sequences\n",
    "print(\"Reviews longer than 80 words : \"+ str(nr_reviews_merged_long))\n",
    "print(\"Reviews in total: \"+ str(len(nr_words_merged)))\n",
    "print(\"Percentage of reviews with more than 80 words: \"+str((nr_reviews_merged_long/len(nr_words_merged))*80)+' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full vocabulary size: 105039\n",
      "Percentage of words covered by vocabulary: 0.9579399973582232\n"
     ]
    }
   ],
   "source": [
    "# determining the vocabulary size\n",
    "\n",
    "# print the full vocabulary size\n",
    "print(\"Full vocabulary size: \" + str(len(word_freqs_merged)))\n",
    "\n",
    "# determine how many words make up the 25000 most commonly used words\n",
    "vocab_words_total_merged = sum(list(word_freqs_merged.values())[:65000])\n",
    "\n",
    "# determine total words \n",
    "words_total_merged = sum(list(word_freqs_merged.values()))\n",
    "\n",
    "# print the percentage of words covered by a vocabulary of 25000\n",
    "print('Percentage of words covered by vocabulary: ' + str(vocab_words_total_merged/words_total_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the vocabulary size and the sequence length\n",
    "vocab_size = 65000\n",
    "seq_len = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding preparation and padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the lookup-table id2word using the vocabulary\n",
    "# id2word maps word ids to written words\n",
    "# PAD token has id 0, UNK token has id 1\n",
    "\n",
    "word2id = {}\n",
    "word2id[\"PAD\"] = 0\n",
    "word2id[\"UNK\"] = 1\n",
    "for v, (k, _) in enumerate(word_freqs_merged.most_common(vocab_size - 2)):\n",
    "    word2id[k] = v + 2\n",
    "id2word = {v:k for k, v in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for checking if a word is contained in the vocabulary, otherwise UNK token\n",
    "\n",
    "\n",
    "# function for loading the word embeddings into the matrix embedding in a sorted way\n",
    "# word embedding for word with id 0 constitutes the starting row in the matrix, etc.\n",
    "def load_vectors(embedding_file, word2id, embed_size):\n",
    "    embedding = np.zeros((len(word2id), embed_size))\n",
    "    file = open(embedding_file, encoding=\"utf8\")\n",
    "    for line in file:\n",
    "        cols = line.strip().split()\n",
    "        word = cols[0]\n",
    "        if embed_size == 0:\n",
    "            embed_size = len(cols) - 1\n",
    "        if word in word2id:\n",
    "            vec = np.array([float(v) for v in cols[1:]])\n",
    "            embedding[lookup_word2id(word)] = vec\n",
    "    embedding[word2id[\"PAD\"]] = np.zeros((embed_size))\n",
    "    embedding[word2id[\"UNK\"]] = unk\n",
    "    # for first time execution:\n",
    "    # embedding[word2id[\"UNK\"]] = np.random.uniform(-1, 1, embed_size)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# function for assigning the word embeddings to the word ids in the sequences\n",
    "# resulting embbedding array emb is of shape (data.shape[0], maxlen, embedding_dim)\n",
    "def assign_wordvectors(parsed_ids, dict_emb):\n",
    "    \n",
    "    emb = np.copy(parsed_ids).astype(int)\n",
    "    emb = list(emb.flatten())\n",
    "    emb = np.array([*map(dict_emb.get, emb)])\n",
    "    emb = emb.reshape((parsed_ids.shape[0], seq_len, 300))\n",
    "    \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the words to ids and pad them for train and test data\n",
    "#parsed_ids_train = [[lookup_word2id(w) for w in s.split()] for s in parsed_reviews_train]\n",
    "#parsed_ids_train = sequence.pad_sequences(parsed_ids_train, 100)\n",
    "parsed_ids_merged = [[lookup_word2id(w) for w in s.split()] for s in parsed_reviews_merged]\n",
    "parsed_ids_merged = sequence.pad_sequences(parsed_ids_merged, 50)\n",
    "parsed_ids_test = [[lookup_word2id(w) for w in s.split()] for s in parsed_reviews_test]\n",
    "parsed_ids_test = sequence.pad_sequences(parsed_ids_test, 50)\n",
    "\n",
    "# load the embeddings into an embedding matrix using the word2id lookup-table\n",
    "# embeddings are loaded exactly in order of the ids\n",
    "embeddings = load_vectors('wiki-news-300d-1M.vec', word2id, 300)\n",
    "\n",
    "# creating an embedding dictionary of word vectors from the embedding matrix\n",
    "# keys are ids and values are word embeddings\n",
    "dict_emb = dict((j, embeddings[j])for j in range(embeddings.shape[0]))\n",
    "\n",
    "# assign word embeddings to train and test data\n",
    "#data_emb_train = assign_wordvectors(parsed_ids_train, dict_emb) \n",
    "data_emb_merged = assign_wordvectors(parsed_ids_merged, dict_emb) \n",
    "data_emb_test = assign_wordvectors(parsed_ids_test, dict_emb) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_emb_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data to files\n",
    "#f = h5py.File('data/fully_preprocessed_data/X_train_preprocessed.h5','a')\n",
    "#f.create_dataset('data',data=data_emb_train)\n",
    "#f.close()\n",
    "h = h5py.File('data/fully_preprocessed_data/X_merged_preprocessed_new14.h5','a')\n",
    "h.create_dataset('data',data=data_emb_merged)\n",
    "h.close()\n",
    "\n",
    "g = h5py.File('data/fully_preprocessed_data/X_test_preprocessed_new14.h5','a')\n",
    "g.create_dataset('data',data=data_emb_test)\n",
    "g.close()\n",
    "\n",
    "\n",
    "#pkl.dump(np.array([y_train,domain_train,idx_domain_train]), open(\"domain_and_label_train.p\", \"wb\"))\n",
    "pkl.dump(np.array([y_merged,domain_merged,idx_domain_merged]), open(\"domain_and_label_merged14.p\", \"wb\"))\n",
    "pkl.dump(np.array([y_test,domain_test,idx_domain_test]), open(\"domain_and_label_test14.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
