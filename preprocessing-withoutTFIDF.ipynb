{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following file contains the code for preprocessing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to create a directory \"data/cleaned_data/\" and \"data/fully_preprocessed_data/\" such that the data can be saved there. Besides, save the fasttext file of https://fasttext.cc/docs/en/english-vectors.html (choosing wiki news - 1 million word vectors) under \"data/fasttext_file/wiki-news-300d-1M.vec\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import nltk\n",
    "import h5py\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of the code below in this file originates from the following book:\n",
    "# Gulli, Antonio, Amita Kapoor, and Sujit Pal. Deep Learning with TensorFlow 2 and Keras: Regression, ConvNets, GANs, RNNs, NLP, and More with TensorFlow 2 and the Keras API. Packt Publishing, Limited, 2019, pp. 365-373."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load cleaned data and the unknown token.\"\"\"\n",
    "    with open('data/cleaned_data/train_cleaned.pkl', 'rb') as f:\n",
    "        df_train = pkl.load(f)\n",
    "    with open('data/cleaned_data/test_cleaned.pkl', 'rb') as f:\n",
    "        df_test = pkl.load(f)\n",
    "    with open('data/cleaned_data/merged_cleaned.pkl', 'rb') as f:\n",
    "        df_merged = pkl.load(f)\n",
    "    with open('unk_token.pkl', 'rb') as f:\n",
    "        unk = pkl.load(f)\n",
    "    return df_train, df_test, df_merged, unk\n",
    "\n",
    "def preprocess_data(df_train, df_test, df_merged):\n",
    "    \"\"\"Nummerate instances and create a dictionary for domain encoding.\"\"\"\n",
    "    df_train['idx_domain'] = df_train.groupby(['domain']).cumcount()\n",
    "    df_test['idx_domain'] = df_test.groupby(['domain']).cumcount()\n",
    "    df_merged['idx_domain'] = df_merged.groupby('domain').cumcount()\n",
    "    dict_domain = dict(zip(sorted(set(np.array(df_train['domain']))), np.arange(16)))\n",
    "    \n",
    "    return dict_domain\n",
    "\n",
    "def divide_data(df, dict_domain):\n",
    "    \"\"\"Divides the dataframe data into reviews, labels, domain, and instance number.\"\"\"\n",
    "    reviews = np.array([s for s in df['text']])\n",
    "    X, y, domain, idx_domain = reviews, np.array(df['label']), np.array(df['domain']), np.array(df['idx_domain'])\n",
    "    domain = [dict_domain.get(c, c) for c in domain]\n",
    "    return X, y, domain, idx_domain\n",
    "\n",
    "def parse_reviews(reviews):\n",
    "    \"\"\"Tokenizes the reviews and counts words.\"\"\"\n",
    "    word_freqs = collections.Counter()\n",
    "    nr_words = []\n",
    "    parsed_reviews = []\n",
    "    for review in reviews:\n",
    "        words = nltk.word_tokenize(review)\n",
    "        word_freqs.update(words)\n",
    "        parsed_reviews.append(\" \".join(words))\n",
    "        nr_words.append(len(words))\n",
    "    return parsed_reviews, nr_words, word_freqs\n",
    "\n",
    "def visualize_data(nr_words_merged):\n",
    "    \"\"\"Visualization: Histogram of review word counts.\"\"\"\n",
    "    plt.hist(nr_words_merged, bins=35, range=(0, 400))\n",
    "    plt.xlabel('Number of words in a review')\n",
    "    plt.ylabel('Number of reviews')\n",
    "    plt.show()\n",
    "\n",
    " def lookup_word2id(word, word2id):\n",
    "    \"\"\"Return word's id if it exists, else return UNK's id.\"\"\"\n",
    "    return word2id.get(word, word2id[\"UNK\"])\n",
    "\n",
    "def load_vectors(embedding_file, word2id, embed_size):\n",
    "    \"\"\"Load word embeddings from a pre-trained embedding file.\"\"\"\n",
    "    embedding = np.zeros((len(word2id), embed_size))\n",
    "    with open(embedding_file, encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            cols = line.strip().split()\n",
    "            word = cols[0]\n",
    "            if word in word2id:\n",
    "                vec = np.array([float(v) for v in cols[1:]])\n",
    "                embedding[lookup_word2id(word, word2id)] = vec\n",
    "    embedding[word2id[\"PAD\"]] = np.zeros((embed_size))\n",
    "    embedding[word2id[\"UNK\"]] = unk\n",
    "    return embedding\n",
    "\n",
    "def assign_wordvectors(parsed_ids, dict_emb):\n",
    "    \"\"\"Assign embeddings to parsed ids.\"\"\"\n",
    "    emb = np.copy(parsed_ids).astype(int)\n",
    "    emb = list(emb.flatten())\n",
    "    emb = np.array([*map(dict_emb.get, emb)])\n",
    "    emb = emb.reshape((parsed_ids.shape[0], seq_len, 300))\n",
    "    return emb\n",
    "\n",
    "def save_to_h5py(data, filepath):\n",
    "    \"\"\"Save data to an h5py file.\"\"\"\n",
    "    with h5py.File(filepath, 'a') as f:\n",
    "        f.create_dataset('data', data=data)   \n",
    "\n",
    "def main():\n",
    "    df_train, df_test, df_merged, unk = load_data()\n",
    "    dict_domain = preprocess_data(df_train, df_test, df_merged)\n",
    "    \n",
    "    X_train, y_train, domain_train, idx_domain_train = divide_data(df_train, dict_domain)\n",
    "    X_test, y_test, domain_test, idx_domain_test = divide_data(df_test, dict_domain)\n",
    "    X_merged, y_merged, domain_merged, idx_domain_merged = divide_data(df_merged, dict_domain)\n",
    "    \n",
    "    parsed_reviews_train, nr_words_train, word_freqs_train = parse_reviews(X_train)\n",
    "    parsed_reviews_test, nr_words_test, _ = parse_reviews(X_test)\n",
    "    parsed_reviews_merged, nr_words_merged, word_freqs_merged = parse_reviews(X_merged)\n",
    "    \n",
    "    # count the reviews that are longer than 100 words\n",
    "    nr_reviews_merged_long = sum(i > 80 for i in nr_words_merged)\n",
    "    visualize_data(nr_words_merged)\n",
    "\n",
    "    # print calculated numbers for longer sequences\n",
    "    print(\"Reviews longer than 80 words : \"+ str(nr_reviews_merged_long))\n",
    "    print(\"Reviews in total: \"+ str(len(nr_words_merged)))\n",
    "    print(\"Percentage of reviews with more than 80 words: \"+str((nr_reviews_merged_long/len(nr_words_merged))*80)+' %')\n",
    "\n",
    "\n",
    "    # determining the vocabulary size\n",
    "\n",
    "    # print the full vocabulary size\n",
    "    print(\"Full vocabulary size: \" + str(len(word_freqs_merged)))\n",
    "\n",
    "    # determine how many words make up the 25000 most commonly used words\n",
    "    vocab_words_total_merged = sum(list(word_freqs_merged.values())[:65000])\n",
    "\n",
    "    # determine total words \n",
    "    words_total_merged = sum(list(word_freqs_merged.values()))\n",
    "\n",
    "    # print the percentage of words covered by a vocabulary of 25000\n",
    "    print('Percentage of words covered by vocabulary: ' + str(vocab_words_total_merged/words_total_merged))\n",
    "\n",
    "    # set the vocabulary size and the sequence length\n",
    "    vocab_size = 65000\n",
    "    seq_len = 50\n",
    "\n",
    "\n",
    "    word2id = {\"PAD\": 0, \"UNK\": 1}\n",
    "    for v, (k, _) in enumerate(word_freqs_merged.most_common(vocab_size - 2)):\n",
    "        word2id[k] = v + 2\n",
    "\n",
    "    parsed_ids_merged = [[lookup_word2id(w, word2id) for w in s.split()] for s in parsed_reviews_merged]\n",
    "    parsed_ids_merged = sequence.pad_sequences(parsed_ids_merged, 50)\n",
    "    parsed_ids_test = [[lookup_word2id(w, word2id) for w in s.split()] for s in parsed_reviews_test]\n",
    "    parsed_ids_test = sequence.pad_sequences(parsed_ids_test, 50)\n",
    "\n",
    "    embeddings = load_vectors('wiki-news-300d-1M.vec', word2id, 300)\n",
    "    dict_emb = {j: embeddings[j] for j in range(embeddings.shape[0])}\n",
    "\n",
    "    data_emb_merged = assign_wordvectors(parsed_ids_merged, dict_emb)\n",
    "    data_emb_test = assign_wordvectors(parsed_ids_test, dict_emb)\n",
    "\n",
    "    save_to_h5py(data_emb_merged, 'data/fully_preprocessed_data/X_merged_preprocessed_new.h5')\n",
    "    save_to_h5py(data_emb_test, 'data/fully_preprocessed_data/X_test_preprocessed_new.h5')\n",
    "\n",
    "    pkl.dump(np.array([y_merged, domain_merged, idx_domain_merged]), open(\"domain_and_label_merged.pkl\", \"wb\"))\n",
    "    pkl.dump(np.array([y_test, domain_test, idx_domain_test]), open(\"domain_and_label_test.pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
